{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cs330_hw4_code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "848f8b0ce97146ad94f4b8fe03dee7f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5890cb4bab06467b80e5628e39f2d744",
              "IPY_MODEL_513c8b20756540969553c0c1cc3e8d90",
              "IPY_MODEL_a4a9c64f8cff49bdba157efebc97bc47"
            ],
            "layout": "IPY_MODEL_edfedf22bb3549899eaedfb5f134bf34"
          }
        },
        "5890cb4bab06467b80e5628e39f2d744": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6547f26062d44527a6b75651867136ff",
            "placeholder": "​",
            "style": "IPY_MODEL_38078b2e31b64ed39b9accf4cd2f4300",
            "value": "  0%"
          }
        },
        "513c8b20756540969553c0c1cc3e8d90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1dba36b42bf40b292910e8d67899c0f",
            "max": 19999,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27416dfc210e4fea871c003576846669",
            "value": 0
          }
        },
        "a4a9c64f8cff49bdba157efebc97bc47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_483c06ac41b64f2f99dd1cf6885fc973",
            "placeholder": "​",
            "style": "IPY_MODEL_051aa1a30d69434e81851bffb8698ce6",
            "value": " 0/19999 [00:00&lt;?, ?it/s]"
          }
        },
        "edfedf22bb3549899eaedfb5f134bf34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6547f26062d44527a6b75651867136ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38078b2e31b64ed39b9accf4cd2f4300": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1dba36b42bf40b292910e8d67899c0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27416dfc210e4fea871c003576846669": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "483c06ac41b64f2f99dd1cf6885fc973": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "051aa1a30d69434e81851bffb8698ce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "abs-a0tivvB_",
        "outputId": "eae9c0da-cb32-44f3-e271-8145c4213bc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# change to directory of the code\n",
        "%cd hw4_code/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\cs330_final_project\\hw4_code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZZx5QPwD0p5"
      },
      "source": [
        "import os\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "# Need to download the Omniglot dataset -- DON'T MODIFY THIS CELL\n",
        "if not os.path.isdir(\"./dream_template\"):\n",
        "    gdd.download_file_from_google_drive(\n",
        "        file_id=\"1O8k6UWSbJOczjQm5-e9g3y1k3FDew1Yn\", dest_path=\"./dream_template.zip\", unzip=True\n",
        "    )\n",
        "    !mv dream_template/* ./\n",
        "\n",
        "required_files = [\n",
        "    \"config.py\",\n",
        "    \"meta_exploration.py\",\n",
        "    \"replay.py\",\n",
        "    \"schedule.py\",\n",
        "    \"policy.py\",\n",
        "    \"requirements.txt\",\n",
        "    \"utils.py\",\n",
        "    \"relabel.py\",\n",
        "    \"rl.py\",\n",
        "    \"wrappers.py\",\n",
        "    \"grid.py\",\n",
        "    \"render.py\",\n",
        "    \"city.py\",\n",
        "]\n",
        "for f in required_files:\n",
        "    assert os.path.isfile(f)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy9Sa3pZEUHW"
      },
      "source": [
        "# !pip install -r -q requirements.txt\n",
        "# !pip install -U -q tensorboard\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U78sWSZATfxX"
      },
      "source": [
        "# embed.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2VeXzosGdLP"
      },
      "source": [
        "# embed.py\n",
        "import abc\n",
        "import collections\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import distributions as td\n",
        "from torch.nn import functional as F\n",
        "import grid\n",
        "import relabel\n",
        "import utils\n",
        "\n",
        "\n",
        "class Embedder(abc.ABC, nn.Module):\n",
        "    \"\"\"Defines the embedding of an object in the forward method.\n",
        "\n",
        "  Subclasses should register to the from_config method.\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim):\n",
        "        \"\"\"Sets the embed dim.\n",
        "\n",
        "    Args:\n",
        "      embed_dim (int): the dimension of the outputted embedding.\n",
        "    \"\"\"\n",
        "        super().__init__()\n",
        "        self._embed_dim = embed_dim\n",
        "\n",
        "    @property\n",
        "    def embed_dim(self):\n",
        "        \"\"\"Returns the dimension of the output (int).\"\"\"\n",
        "        return self._embed_dim\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        \"\"\"Constructs and returns Embedder from config.\n",
        "\n",
        "    Args:\n",
        "      config (Config): parameters for constructing the Embedder.\n",
        "\n",
        "    Returns:\n",
        "      Embedder\n",
        "    \"\"\"\n",
        "        config_type = config.get(\"type\")\n",
        "        if config_type == \"simple_grid_state\":\n",
        "            return SimpleGridStateEmbedder.from_config(config)\n",
        "        elif config_type == \"fixed_vocab\":\n",
        "            return FixedVocabEmbedder.from_config(config)\n",
        "        elif config_type == \"linear\":\n",
        "            return LinearEmbedder.from_config(config)\n",
        "        else:\n",
        "            raise ValueError(\"Config type {} not supported\".format(config_type))\n",
        "\n",
        "\n",
        "def get_state_embedder(env):\n",
        "    \"\"\"Returns the appropriate type of embedder given the environment type.\"\"\"\n",
        "    env = env.unwrapped\n",
        "    if isinstance(env.unwrapped, grid.GridEnv):\n",
        "        return SimpleGridStateEmbedder\n",
        "\n",
        "    # Dependencies on OpenGL, so only load if absolutely necessary\n",
        "    from envs.miniworld import sign\n",
        "\n",
        "    if isinstance(env, sign.MiniWorldSign):\n",
        "        return MiniWorldEmbedder\n",
        "\n",
        "    raise ValueError()\n",
        "\n",
        "\n",
        "class TransitionEmbedder(Embedder):\n",
        "    def __init__(self, state_embedder, action_embedder, reward_embedder, embed_dim):\n",
        "        super().__init__(embed_dim)\n",
        "\n",
        "        self._state_embedder = state_embedder\n",
        "        self._action_embedder = action_embedder\n",
        "        self._reward_embedder = reward_embedder\n",
        "        reward_embed_dim = 0 if reward_embedder is None else reward_embedder.embed_dim\n",
        "\n",
        "        self._transition_embedder = nn.Sequential(\n",
        "            nn.Linear(self._state_embedder.embed_dim * 2 + self._action_embedder.embed_dim + reward_embed_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, embed_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, experiences):\n",
        "        state_embeds = self._state_embedder([exp.state.observation for exp in experiences])\n",
        "        next_state_embeds = self._state_embedder([exp.next_state.observation for exp in experiences])\n",
        "        action_embeds = self._action_embedder([exp.action for exp in experiences])\n",
        "        embeddings = [state_embeds, next_state_embeds, action_embeds]\n",
        "        if self._reward_embedder is not None:\n",
        "            embeddings.append(self._reward_embedder([exp.next_state.prev_reward for exp in experiences]))\n",
        "        transition_embeds = self._transition_embedder(torch.cat(embeddings, -1))\n",
        "        return transition_embeds\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config, env):\n",
        "        state_embedder = get_state_embedder(env)(\n",
        "            env.observation_space[\"observation\"], config.get(\"experience_embedder\").get(\"state_embed_dim\")\n",
        "        )\n",
        "        action_embedder = FixedVocabEmbedder(\n",
        "            env.action_space.n, config.get(\"experience_embedder\").get(\"action_embedder\").get(\"embed_dim\")\n",
        "        )\n",
        "        return cls(state_embedder, action_embedder, config.get(\"embed_dim\"))\n",
        "\n",
        "\n",
        "class TrajectoryEmbedder(Embedder, relabel.RewardLabeler):\n",
        "    def __init__(self, transition_embedder, id_embedder, penalty, embed_dim):\n",
        "        super().__init__(embed_dim)\n",
        "\n",
        "        self._transition_embedder = transition_embedder\n",
        "        self._id_embedder = id_embedder\n",
        "        self._transition_lstm = nn.LSTM(transition_embedder.embed_dim, 128)\n",
        "        self._transition_fc_layer = nn.Linear(128, 128)\n",
        "        self._transition_output_layer = nn.Linear(128, embed_dim)\n",
        "        self._penalty = penalty\n",
        "        self._use_ids = True\n",
        "\n",
        "    def use_ids(self, use):\n",
        "        self._use_ids = use\n",
        "\n",
        "    def _compute_contexts(self, trajectories):\n",
        "        \"\"\"Returns contexts and masks.\n",
        "\n",
        "    Args:\n",
        "      trajectories (list[list[Experience]]): see forward().\n",
        "\n",
        "    Returns:\n",
        "      id_contexts (torch.FloatTensor): tensor of shape (batch_size, embed_dim)\n",
        "        embedding the id's in the trajectories.\n",
        "      all_transition_contexts (torch.FloatTensor): tensor of shape\n",
        "        (batch_size, max_len + 1, embed_dim) embedding the sequences of states\n",
        "        and actions in the trajectories.\n",
        "      transition_contexts (torch.FloatTensor): tensor of shape\n",
        "        (batch_size, embed_dim) equal to the last unpadded value in\n",
        "        all_transition_contexts.\n",
        "      mask (torch.BoolTensor): tensor of shape (batch_size, max_len + 1).\n",
        "        The value is False if the trajectory_contexts value should be masked.\n",
        "    \"\"\"\n",
        "        # trajectories: (batch_size, max_len)\n",
        "        # mask: (batch_size, max_len)\n",
        "        padded_trajectories, mask = utils.pad(trajectories)\n",
        "        sequence_lengths = torch.tensor([len(traj) for traj in trajectories]).long()\n",
        "\n",
        "        # (batch_size * max_len, embed_dim)\n",
        "        transition_embed = self._transition_embedder([exp for traj in padded_trajectories for exp in traj])\n",
        "\n",
        "        # pack_padded_sequence relies on the default tensor type not\n",
        "        # being a CUDA tensor.\n",
        "        torch.set_default_tensor_type(torch.FloatTensor)\n",
        "        # Sorted only required for ONNX\n",
        "        padded_transitions = nn.utils.rnn.pack_padded_sequence(\n",
        "            transition_embed.reshape(mask.shape[0], mask.shape[1], -1),\n",
        "            sequence_lengths,\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False,\n",
        "        )\n",
        "        if torch.cuda.is_available():\n",
        "            torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "\n",
        "        transition_hidden_states = self._transition_lstm(padded_transitions)[0]\n",
        "        # (batch_size, max_len, hidden_dim)\n",
        "        transition_hidden_states, hidden_lengths = nn.utils.rnn.pad_packed_sequence(\n",
        "            transition_hidden_states, batch_first=True\n",
        "        )\n",
        "        initial_hidden_states = torch.zeros(transition_hidden_states.shape[0], 1, transition_hidden_states.shape[-1])\n",
        "        # (batch_size, max_len + 1, hidden_dim)\n",
        "        transition_hidden_states = torch.cat((initial_hidden_states, transition_hidden_states), 1)\n",
        "        transition_hidden_states = F.relu(self._transition_fc_layer(transition_hidden_states))\n",
        "        # (batch_size, max_len + 1, embed_dim)\n",
        "        all_transition_contexts = self._transition_output_layer(transition_hidden_states)\n",
        "\n",
        "        # (batch_size, 1, embed_dim)\n",
        "        # Don't need to subtract 1 off of hidden_lengths as transition_contexts is\n",
        "        # padded with init hidden state at the beginning.\n",
        "        indices = (\n",
        "            hidden_lengths.unsqueeze(-1)\n",
        "            .unsqueeze(-1)\n",
        "            .expand(hidden_lengths.shape[0], 1, all_transition_contexts.shape[2])\n",
        "            .to(all_transition_contexts.device)\n",
        "        )\n",
        "        transition_contexts = all_transition_contexts.gather(1, indices).squeeze(1)\n",
        "\n",
        "        # (batch_size, embed_dim)\n",
        "        id_contexts = self._id_embedder(torch.tensor([traj[0].state.env_id for traj in trajectories]))\n",
        "\n",
        "        # don't mask the initial hidden states (batch_size, max_len + 1)\n",
        "        mask = torch.cat((torch.ones(transition_contexts.shape[0], 1).bool(), mask), -1)\n",
        "        return id_contexts, all_transition_contexts, transition_contexts, mask\n",
        "\n",
        "    def _compute_losses(self, trajectories, id_contexts, all_transition_contexts, transition_contexts, mask):\n",
        "        \"\"\"Computes losses based on the return values of _compute_contexts.\n",
        "\n",
        "    Args:\n",
        "      See return values of _compute_contexts.\n",
        "\n",
        "    Returns:\n",
        "      losses (dict(str: torch.FloatTensor)): see forward().\n",
        "    \"\"\"\n",
        "        del trajectories\n",
        "\n",
        "        ##################### TODO - ADD YOUR CODE HERE  #########################\n",
        "        # the loss is the NLL of posterior of the decoder (wrt output of the encoder)\n",
        "        # mask refers to the true identifier\n",
        "        # need something like tf.tile (or tf.expand_dims and broadcast?)\n",
        "        # tf.stop_gradient(tf.tile(id_contexts,...)) since we need to backprop the loss of id_contexts to the encoder\n",
        "        # id_contexts is the output of the encoder\n",
        "        _id_contexts = id_contexts.unsqueeze(1).expand_as(all_transition_contexts).detach()\n",
        "        # then sum L2 loss at the last dimension\n",
        "        transition_context_loss = ((all_transition_contexts - _id_contexts) ** 2).sum(-1)\n",
        "        ##########################################################################\n",
        "        transition_context_loss = (transition_context_loss * mask).sum() / mask.sum()\n",
        "\n",
        "        cutoff = torch.ones(id_contexts.shape[0]) * 10\n",
        "\n",
        "        losses = {\n",
        "            \"transition_context_loss\": transition_context_loss,\n",
        "            \"id_context_loss\": torch.max((id_contexts ** 2).sum(-1), cutoff).mean(),\n",
        "        }\n",
        "        return losses\n",
        "\n",
        "    def forward(self, trajectories):\n",
        "        \"\"\"Embeds a batch of trajectories.\n",
        "\n",
        "    Args:\n",
        "      trajectories (list[list[Experience]]): batch of trajectories, where each\n",
        "        trajectory comes from the same episode.\n",
        "\n",
        "    Returns:\n",
        "      embedding (torch.FloatTensor): tensor of shape (batch_size, embed_dim)\n",
        "        embedding the trajectories. This embedding is based on the ids if\n",
        "        use_ids is True, otherwise based on the transitions.\n",
        "      losses (dict(str: torch.FloatTensor)): maps auxiliary loss names to their\n",
        "        values.\n",
        "    \"\"\"\n",
        "        id_contexts, all_transition_contexts, transition_contexts, mask = self._compute_contexts(trajectories)\n",
        "        contexts = id_contexts + 0.1 * torch.randn_like(id_contexts) if self._use_ids else transition_contexts\n",
        "        losses = self._compute_losses(trajectories, id_contexts, all_transition_contexts, transition_contexts, mask)\n",
        "        return contexts, losses\n",
        "\n",
        "    def label_rewards(self, trajectories):\n",
        "        \"\"\"Computes rewards for each experience in the trajectory.\n",
        "\n",
        "    Args:\n",
        "      trajectories (list[list[Experience]]): batch of trajectories.\n",
        "\n",
        "    Returns:\n",
        "      rewards (torch.FloatTensor): of shape (batch_size, max_seq_len) where\n",
        "        rewards[i][j] is the rewards for the experience trajectories[i][j].\n",
        "        This is padded with zeros and is detached from the graph.\n",
        "      distances (torch.FloatTensor): of shape (batch_size, max_seq_len + 1)\n",
        "        equal to ||f(e) - g(\\tau^e_{:t})|| for each t.\n",
        "    \"\"\"\n",
        "        id_contexts, all_transition_contexts, _, mask = self._compute_contexts(trajectories)\n",
        "        ##################### TODO - ADD YOUR CODE HERE  #########################\n",
        "        # id_contexts (torch.FloatTensor): tensor of shape (batch_size, embed_dim)\n",
        "        #   embedding the id's in the trajectories.\n",
        "        # all_transition_contexts (torch.FloatTensor): tensor of shape\n",
        "        #   (batch_size, max_len + 1, embed_dim) embedding the sequences of states\n",
        "        #   and actions in the trajectories.\n",
        "\n",
        "        # notice that the original paper also includes a small penalty c to encourage exploration in fewer steps\n",
        "        # the reward would be temporal difference of the log-likelihood of the decoder (wrt output of the encoder)\n",
        "        # since the decoder is modelled as a Gaussian, the log-likelihood is a square differnce between  (+ some constant)\n",
        "        _id_contexts = id_contexts.unsqueeze(1).expand_as(all_transition_contexts).detach()\n",
        "        distances = ((all_transition_contexts - _id_contexts) ** 2).sum(-1)\n",
        "        # the reward is the differences of distances (max_len + 1 coz zero padded)\n",
        "        rewards = -distances[:, 1:] + distances[:, :-1] - self._penalty\n",
        "        ##########################################################################\n",
        "        return (rewards * mask[:, 1:]).detach(), distances\n",
        "\n",
        "\n",
        "class InstructionPolicyEmbedder(Embedder):\n",
        "    \"\"\"Embeds (s, i, \\tau^e) where:\n",
        "\n",
        "    - s is the current state\n",
        "    - i is the current instruction\n",
        "    - \\tau^e is an exploration trajectory (s_0, a_0, s_1, ..., s_T)\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, trajectory_embedder, obs_embedder, instruction_embedder, embed_dim):\n",
        "        \"\"\"Constructs around embedders for each component.\n",
        "\n",
        "    Args:\n",
        "      trajectory_embedder (TrajectoryEmbedder): embeds batches of \\tau^e\n",
        "        (list[list[rl.Experience]]).\n",
        "      obs_embedder (Embedder): embeds batches of states s.\n",
        "      instruction_embedder (Embedder): embeds batches of instructions i. (and the problem ID)\n",
        "      embed_dim (int): see Embedder.\n",
        "    \"\"\"\n",
        "        super().__init__(embed_dim)\n",
        "\n",
        "        self._obs_embedder = obs_embedder\n",
        "        self._instruction_embedder = instruction_embedder\n",
        "        self._trajectory_embedder = trajectory_embedder\n",
        "        self._fc_layer = nn.Linear(\n",
        "            obs_embedder.embed_dim + instruction_embedder.embed_dim + self._trajectory_embedder.embed_dim, 256\n",
        "        )\n",
        "        self._final_layer = nn.Linear(256, embed_dim)\n",
        "\n",
        "    def forward(self, states, hidden_state):\n",
        "        del hidden_state\n",
        "\n",
        "        ##################### TODO - ADD YOUR CODE HERE  #########################\n",
        "        # \"states\" do not refer to the history of states of the agent; see class ExperienceEmbedder for its shape\n",
        "        # please make this easier {{{(>_<)}}} ___*( ￣皿￣)/#____\n",
        "        observations = [state.observation for state in states]\n",
        "        instructions = [torch.tensor(state.instructions) for state in states]\n",
        "        trajectory = [state.trajectory for state in states]\n",
        "\n",
        "        observation_embeddings = self._obs_embedder(observations)\n",
        "        instruction_embeddings = self._instruction_embedder(instructions)\n",
        "        trajectory_embeddings, aux_losses = self._trajectory_embedder(trajectory)\n",
        "\n",
        "        # concat them side by side\n",
        "        input_embeddings = torch.cat((observation_embeddings, instruction_embeddings, trajectory_embeddings), dim=-1)\n",
        "        ##########################################################################\n",
        "        hidden = F.relu(self._fc_layer(input_embeddings))\n",
        "        return self._final_layer(hidden), aux_losses\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config, env):\n",
        "        \"\"\"Returns a configured InstructionPolicyEmbedder.\n",
        "\n",
        "    Args:\n",
        "      config (Config): see Embedder.from_config.\n",
        "      env (gym.Wrapper): the environment to run on. Expects this to be wrapped\n",
        "        with an InstructionWrapper.\n",
        "\n",
        "    Returns:\n",
        "      InstructionPolicyEmbedder: configured according to config.\n",
        "    \"\"\"\n",
        "        obs_embedder = get_state_embedder(env)(\n",
        "            env.observation_space[\"observation\"], config.get(\"obs_embedder\").get(\"embed_dim\")\n",
        "        )\n",
        "        # Use SimpleGridEmbeder since these are just discrete vars\n",
        "        instruction_embedder = SimpleGridStateEmbedder(\n",
        "            env.observation_space[\"instructions\"], config.get(\"instruction_embedder\").get(\"embed_dim\")\n",
        "        )\n",
        "\n",
        "        transition_config = config.get(\"transition_embedder\")\n",
        "        state_embedder = get_state_embedder(env)(\n",
        "            env.observation_space[\"observation\"], transition_config.get(\"state_embed_dim\")\n",
        "        )\n",
        "        action_embedder = FixedVocabEmbedder(env.action_space.n, transition_config.get(\"action_embed_dim\"))\n",
        "        reward_embedder = None\n",
        "        if transition_config.get(\"reward_embed_dim\") is not None:\n",
        "            reward_embedder = LinearEmbedder(1, transition_config.get(\"reward_embed_dim\"))\n",
        "        transition_embedder = TransitionEmbedder(\n",
        "            state_embedder, action_embedder, reward_embedder, transition_config.get(\"embed_dim\")\n",
        "        )\n",
        "        id_embedder = IDEmbedder(\n",
        "            env.observation_space[\"env_id\"].high, config.get(\"transition_embedder\").get(\"embed_dim\")\n",
        "        )\n",
        "        if config.get(\"trajectory_embedder\").get(\"type\") == \"ours\":\n",
        "            trajectory_embedder = TrajectoryEmbedder(\n",
        "                transition_embedder,\n",
        "                id_embedder,\n",
        "                config.get(\"trajectory_embedder\").get(\"penalty\"),\n",
        "                transition_embedder.embed_dim,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported trajectory embedder {}\".format(config.get(\"trajectory_embedder\")))\n",
        "        return cls(trajectory_embedder, obs_embedder, instruction_embedder, config.get(\"embed_dim\"))\n",
        "\n",
        "\n",
        "class RecurrentAndTaskIDEmbedder(Embedder):\n",
        "    \"\"\"Embedding used by IMPORT.\n",
        "\n",
        "  Compute both:\n",
        "    - g(\\tau_{:t}) recurrently\n",
        "    - f(e)\n",
        "\n",
        "  Full embedding is:\n",
        "    \\phi(s_t, z), where z is randomly chosen from g(\\tau_{:t}) and f(e).\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, recurrent_state_embedder, id_embedder, state_embedder, embed_dim):\n",
        "        super().__init__(embed_dim)\n",
        "        assert id_embedder.embed_dim == recurrent_state_embedder.embed_dim\n",
        "        self._recurrent_state_embedder = recurrent_state_embedder\n",
        "        self._id_embedder = id_embedder\n",
        "        self._state_embedder = state_embedder\n",
        "        self._final_layer = nn.Linear(id_embedder.embed_dim + state_embedder.embed_dim, embed_dim)\n",
        "        self._use_id = False\n",
        "\n",
        "    def use_ids(self, use):\n",
        "        self._use_id = use\n",
        "\n",
        "    def _compute_embeddings(self, states, hidden_state=None):\n",
        "        # (batch_size, seq_len, embed_dim)\n",
        "        recurrent_embedding, hidden_state = self._recurrent_state_embedder(states, hidden_state)\n",
        "        # (batch_size, embed_dim)\n",
        "        id_embedding = self._id_embedder(torch.tensor([seq[0].env_id for seq in states]))\n",
        "\n",
        "        if len(recurrent_embedding.shape) > 2:\n",
        "            id_embedding = id_embedding.unsqueeze(1).expand_as(recurrent_embedding)\n",
        "        return recurrent_embedding, id_embedding, hidden_state\n",
        "\n",
        "    def forward(self, states, hidden_state=None):\n",
        "        recurrent_embedding, id_embedding, hidden_state = self._compute_embeddings(states, hidden_state)\n",
        "\n",
        "        history_embed = recurrent_embedding\n",
        "        if self._use_id:\n",
        "            history_embed = id_embedding\n",
        "\n",
        "        # (batch_size, seq_len, state_embed_dim) or (batch_size, state_embed_dim)\n",
        "        state_embeds = self._state_embedder([state for seq in states for state in seq])\n",
        "        if len(history_embed.shape) > 2:\n",
        "            state_embeds = state_embeds.reshape(history_embed.shape[0], history_embed.shape[1], -1)\n",
        "        return self._final_layer(F.relu(torch.cat((history_embed, state_embeds), -1))), hidden_state\n",
        "\n",
        "    def aux_loss(self, trajectories):\n",
        "        # (batch_size, max_seq_len)\n",
        "        trajectories, mask = utils.pad(trajectories)\n",
        "\n",
        "        # (batch_size, max_seq_len, embed_dim)\n",
        "        recurrent_embeddings, id_embeddings, hidden_state = self._compute_embeddings(\n",
        "            [[exp.state for exp in traj] for traj in trajectories], [traj[0].agent_state for traj in trajectories]\n",
        "        )\n",
        "\n",
        "        return {\"embedding_distance\": (((recurrent_embeddings - id_embeddings.detach()) ** 2).mean(0).sum())}\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config, env):\n",
        "        recurrent_state_embedder = RecurrentStateEmbedder.from_config(config.get(\"recurrent_embedder\"), env)\n",
        "        state_embed_config = config.get(\"state_embedder\")\n",
        "        state_embedder = get_state_embedder(env)(\n",
        "            env.observation_space[\"observation\"], state_embed_config.get(\"embed_dim\")\n",
        "        )\n",
        "        instruction_embedder = SimpleGridStateEmbedder(\n",
        "            env.observation_space[\"instructions\"], state_embed_config.get(\"embed_dim\")\n",
        "        )\n",
        "        state_embedder = StateInstructionEmbedder(\n",
        "            state_embedder, instruction_embedder, state_embed_config.get(\"embed_dim\")\n",
        "        )\n",
        "\n",
        "        id_embed_config = config.get(\"id_embedder\")\n",
        "        id_embedder = IDEmbedder(env.observation_space[\"env_id\"].high, id_embed_config.get(\"embed_dim\"))\n",
        "        return cls(recurrent_state_embedder, id_embedder, state_embedder, config.get(\"embed_dim\"))\n",
        "\n",
        "\n",
        "class VariBADEmbedder(Embedder):\n",
        "    \"\"\"Embedding used by VariBAD.\n",
        "\n",
        "  Computes:\n",
        "    - g(\\tau_{:t}) recurrently and applies fully connected heads on top to\n",
        "      produce q(z_t | \\tau_{:t}) = N(head1(g(\\tau_{:t})), head2(g(\\tau_{:t})))\n",
        "    - embedding = \\phi(z_t.detach(), embed(s_t))\n",
        "\n",
        "  Decoding auxiliary loss:\n",
        "    - \\sum_t \\sum_i ||decoder(z_i, e(s_t), e(a_t)) - r_t||_2^2\n",
        "    - \\sum_t \\sum_i ||decoder(z_i, e(s_t), e(a_t)) - s_{t + 1}||_2^2\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        recurrent_state_embedder,\n",
        "        z_dim,\n",
        "        state_embedder,\n",
        "        action_embedder,\n",
        "        state_dim,\n",
        "        embed_dim,\n",
        "        predict_state=True,\n",
        "    ):\n",
        "        super().__init__(embed_dim)\n",
        "        self._recurrent_state_embedder = recurrent_state_embedder\n",
        "        self._fc_mu = nn.Linear(recurrent_state_embedder.embed_dim, z_dim)\n",
        "        self._fc_logvar = nn.Linear(recurrent_state_embedder.embed_dim, z_dim)\n",
        "        self._state_embedder = state_embedder\n",
        "        self._phi = nn.Linear(z_dim + state_embedder.embed_dim, embed_dim)\n",
        "        self._action_embedder = action_embedder\n",
        "        self._decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim + state_embedder.embed_dim + action_embedder.embed_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "        )\n",
        "\n",
        "        # Predicts reward / state\n",
        "        self._reward_head = nn.Linear(128, 1)\n",
        "        self._state_head = nn.Linear(128, state_dim)\n",
        "\n",
        "        # If False, does not do state prediction\n",
        "        self._predict_state = predict_state\n",
        "        self._z_dim = z_dim\n",
        "\n",
        "    def _compute_z_distr(self, states, hidden_state=None):\n",
        "        embeddings, hidden_state = self._recurrent_state_embedder(states, hidden_state=hidden_state)\n",
        "\n",
        "        # (batch_size, sequence_length, embed_dim)\n",
        "        mu = embeddings\n",
        "        std = torch.ones_like(mu) * 1e-6\n",
        "\n",
        "        q = td.Independent(td.Normal(mu, std), 1)\n",
        "        return q, hidden_state\n",
        "\n",
        "    def forward(self, states, hidden_state=None):\n",
        "        q, hidden_state = self._compute_z_distr(states, hidden_state)\n",
        "        # Don't backprop through encoder\n",
        "        z = q.rsample()\n",
        "\n",
        "        # (batch_size, seq_len, state_embed_dim) or (batch_size, state_embed_dim)\n",
        "        state_embeds = self._state_embedder([state for seq in states for state in seq])\n",
        "        if len(z.shape) > 2:\n",
        "            state_embeds = state_embeds.reshape(z.shape[0], z.shape[1], -1)\n",
        "        return self._phi(F.relu(torch.cat((z, state_embeds), -1))), hidden_state\n",
        "\n",
        "    def aux_loss(self, trajectories):\n",
        "        # The trajectories that we will try to decode\n",
        "        # (batch_size, max_trajectory_len)\n",
        "        trajectories_to_predict, predict_mask = utils.pad([traj[0].trajectory for traj in trajectories])\n",
        "\n",
        "        # The trajectories we're using to encode z\n",
        "        # They differ when we sample not the full trajectory\n",
        "        # (batch_size, max_sequence_len)\n",
        "        padded_trajectories, mask = utils.pad(trajectories)\n",
        "\n",
        "        q = self._compute_z_distr(\n",
        "            [[exp.state for exp in traj] for traj in padded_trajectories],\n",
        "            [traj[0].agent_state for traj in padded_trajectories],\n",
        "        )[0]\n",
        "        # (batch_size, max_sequence_len, z_dim)\n",
        "        z = q.rsample()\n",
        "        # (batch_size, max_trajectory_len, max_sequence_len, z_dim)\n",
        "        z = z.unsqueeze(1).expand(-1, predict_mask.shape[1], -1, -1)\n",
        "\n",
        "        # (batch_size, max_trajectory_len, embed_dim)\n",
        "        # e(s)\n",
        "        state_embeds = self._state_embedder(\n",
        "            [exp.state for trajectory in trajectories_to_predict for exp in trajectory]\n",
        "        ).reshape(z.shape[0], z.shape[1], -1)\n",
        "        # e(a)\n",
        "        action_embeds = self._action_embedder(\n",
        "            [exp.action for trajectory in trajectories_to_predict for exp in trajectory]\n",
        "        ).reshape(z.shape[0], z.shape[1], -1)\n",
        "\n",
        "        # (batch_size, max_trajectory_len, max_sequence_len, embed_dim)\n",
        "        state_embeds = state_embeds.unsqueeze(2).expand(-1, -1, z.shape[2], -1)\n",
        "        action_embeds = action_embeds.unsqueeze(2).expand(-1, -1, z.shape[2], -1)\n",
        "\n",
        "        decoder_input = torch.cat((z, state_embeds, action_embeds), -1)\n",
        "        decoder_embed = self._decoder(decoder_input)\n",
        "\n",
        "        # (batch_size, max_trajectory_len, max_sequence_len, 1)\n",
        "        predicted_rewards = self._reward_head(F.relu(decoder_embed))\n",
        "\n",
        "        # (batch_size, max_trajectory_len)\n",
        "        true_rewards = torch.tensor(\n",
        "            [[exp.next_state.prev_reward for exp in trajectory] for trajectory in trajectories_to_predict]\n",
        "        )\n",
        "\n",
        "        # (batch_size, max_trajectory_len, max_sequence_len, 1)\n",
        "        true_rewards = true_rewards.unsqueeze(-1).unsqueeze(-1).expand_as(predicted_rewards)\n",
        "\n",
        "        # (batch_size, max_trajectory_len, max_sequence_len, 1)\n",
        "        reward_decoding_loss = (predicted_rewards - true_rewards) ** 2\n",
        "\n",
        "        predict_mask = predict_mask.unsqueeze(2).expand(-1, -1, mask.shape[-1])\n",
        "        mask = mask.unsqueeze(1).expand_as(predict_mask)\n",
        "        # (batch_size, max_trajectory_len, max_sequence_len, 1)\n",
        "        aggregate_mask = (predict_mask * mask).unsqueeze(-1)\n",
        "        reward_decoding_loss = (reward_decoding_loss * aggregate_mask).sum() / reward_decoding_loss.shape[0]\n",
        "\n",
        "        state_decoding_loss = torch.tensor(0).float()\n",
        "        if self._predict_state:\n",
        "            # (batch_size, max_trajectory_len, max_sequence_len, state_dim)\n",
        "            predicted_states = self._state_head(F.relu(decoder_embed))\n",
        "\n",
        "            # (batch_size, max_trajectory_len, state_dim)\n",
        "            next_states_to_predict = torch.stack(\n",
        "                [\n",
        "                    torch.stack([exp.next_state.observation for exp in trajectory])\n",
        "                    for trajectory in trajectories_to_predict\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            # (batch_size, max_trajectory_len, max_sequence_len, state_dim)\n",
        "            next_states_to_predict = next_states_to_predict.unsqueeze(2).expand_as(predicted_states)\n",
        "\n",
        "            # (batch_size, max_trajectory_len, max_sequence_len, state_dim)\n",
        "            state_decoding_loss = (predicted_states - next_states_to_predict) ** 2\n",
        "            state_decoding_loss = (state_decoding_loss * aggregate_mask).sum() / state_decoding_loss.shape[0]\n",
        "\n",
        "        # kl_loss = td.kl_divergence(q, self._prior(mask.shape[0], mask.shape[1]))\n",
        "        return {\n",
        "            \"reward_decoding_loss\": reward_decoding_loss,\n",
        "            \"state_decoding_loss\": state_decoding_loss * 0.01,\n",
        "            # \"kl_loss\": kl_loss * 0.1,\n",
        "        }\n",
        "\n",
        "    def _prior(self, batch_size, sequence_len):\n",
        "        mu = torch.zeros(batch_size, sequence_len, self._z_dim)\n",
        "        std = torch.ones_like(mu)\n",
        "        return td.Independent(td.Normal(mu, std), 1)\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config, env):\n",
        "        recurrent_state_embedder = RecurrentStateEmbedder.from_config(config.get(\"recurrent_embedder\"), env)\n",
        "        state_embed_config = config.get(\"state_embedder\")\n",
        "        state_embedder = get_state_embedder(env)(\n",
        "            env.observation_space[\"observation\"], state_embed_config.get(\"embed_dim\")\n",
        "        )\n",
        "        instruction_embedder = SimpleGridStateEmbedder(\n",
        "            env.observation_space[\"instructions\"], state_embed_config.get(\"embed_dim\")\n",
        "        )\n",
        "        state_embedder = StateInstructionEmbedder(\n",
        "            state_embedder, instruction_embedder, state_embed_config.get(\"embed_dim\")\n",
        "        )\n",
        "\n",
        "        action_embed_config = config.get(\"action_embedder\")\n",
        "        action_embedder = FixedVocabEmbedder(env.action_space.n, action_embed_config.get(\"embed_dim\"))\n",
        "        state_dim = len(env.observation_space[\"observation\"].high)\n",
        "        return cls(\n",
        "            recurrent_state_embedder,\n",
        "            config.get(\"z_dim\"),\n",
        "            state_embedder,\n",
        "            action_embedder,\n",
        "            state_dim,\n",
        "            config.get(\"embed_dim\"),\n",
        "            config.get(\"predict_states\"),\n",
        "        )\n",
        "\n",
        "\n",
        "class RecurrentStateEmbedder(Embedder):\n",
        "    \"\"\"Applies an LSTM on top of a state embedding.\"\"\"\n",
        "\n",
        "    def __init__(self, state_embedder, embed_dim):\n",
        "        super().__init__(embed_dim)\n",
        "\n",
        "        self._state_embedder = state_embedder\n",
        "        self._lstm_cell = nn.LSTMCell(state_embedder.embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, states, hidden_state=None):\n",
        "        \"\"\"Embeds a batch of sequences of contiguous states.\n",
        "\n",
        "    Args:\n",
        "      states (list[list[np.array]]): of shape\n",
        "        (batch_size, sequence_length, state_dim).\n",
        "      hidden_state (list[object] | None): batch of initial hidden states\n",
        "        to use with the LSTM. During inference, this should just be the\n",
        "        previously returned hidden state.\n",
        "\n",
        "    Returns:\n",
        "      embedding (torch.tensor): shape (batch_size, sequence_length, embed_dim)\n",
        "      hidden_state (object): hidden state after embedding every element in the\n",
        "        sequence.\n",
        "    \"\"\"\n",
        "        batch_size = len(states)\n",
        "        sequence_len = len(states[0])\n",
        "\n",
        "        # Stack batched hidden state\n",
        "        if batch_size > 1 and hidden_state is not None:\n",
        "            hs = []\n",
        "            cs = []\n",
        "            for hidden in hidden_state:\n",
        "                if hidden is None:\n",
        "                    hs.append(torch.zeros(1, self.embed_dim))\n",
        "                    cs.append(torch.zeros(1, self.embed_dim))\n",
        "                else:\n",
        "                    hs.append(hidden[0])\n",
        "                    cs.append(hidden[1])\n",
        "            hidden_state = (torch.cat(hs, 0), torch.cat(cs, 0))\n",
        "\n",
        "        flattened = [state for seq in states for state in seq]\n",
        "\n",
        "        # (batch_size * sequence_len, embed_dim)\n",
        "        state_embeds = self._state_embedder(flattened)\n",
        "        state_embeds = state_embeds.reshape(batch_size, sequence_len, -1)\n",
        "\n",
        "        embeddings = []\n",
        "        for seq_index in range(sequence_len):\n",
        "            hidden_state = self._lstm_cell(state_embeds[:, seq_index, :], hidden_state)\n",
        "\n",
        "            # (batch_size, 1, embed_dim)\n",
        "            embeddings.append(hidden_state[0].unsqueeze(1))\n",
        "\n",
        "        # (batch_size, sequence_len, embed_dim)\n",
        "        # squeezed to (batch_size, embed_dim) if sequence_len == 1\n",
        "        embeddings = torch.cat(embeddings, 1).squeeze(1)\n",
        "\n",
        "        # Detach to save GPU memory.\n",
        "        detached_hidden_state = (hidden_state[0].detach(), hidden_state[1].detach())\n",
        "        return embeddings, detached_hidden_state\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config, env):\n",
        "        experience_embed_config = config.get(\"experience_embedder\")\n",
        "        state_embedder = get_state_embedder(env)(\n",
        "            env.observation_space[\"observation\"], experience_embed_config.get(\"state_embed_dim\")\n",
        "        )\n",
        "        action_embedder = FixedVocabEmbedder(env.action_space.n + 1, experience_embed_config.get(\"action_embed_dim\"))\n",
        "        instruction_embedder = None\n",
        "        if experience_embed_config.get(\"instruction_embed_dim\") is not None:\n",
        "            # Use SimpleGridEmbedder since these are just discrete vars\n",
        "            instruction_embedder = SimpleGridStateEmbedder(\n",
        "                env.observation_space[\"instructions\"], experience_embed_config.get(\"instruction_embed_dim\")\n",
        "            )\n",
        "\n",
        "        reward_embedder = None\n",
        "        if experience_embed_config.get(\"reward_embed_dim\") is not None:\n",
        "            reward_embedder = LinearEmbedder(1, experience_embed_config.get(\"reward_embed_dim\"))\n",
        "\n",
        "        done_embedder = None\n",
        "        if experience_embed_config.get(\"done_embed_dim\") is not None:\n",
        "            done_embedder = FixedVocabEmbedder(2, experience_embed_config.get(\"done_embed_dim\"))\n",
        "\n",
        "        experience_embedder = ExperienceEmbedder(\n",
        "            state_embedder,\n",
        "            instruction_embedder,\n",
        "            action_embedder,\n",
        "            reward_embedder,\n",
        "            done_embedder,\n",
        "            experience_embed_config.get(\"embed_dim\"),\n",
        "        )\n",
        "        return cls(experience_embedder, config.get(\"embed_dim\"))\n",
        "\n",
        "\n",
        "class StateInstructionEmbedder(Embedder):\n",
        "    \"\"\"Embeds instructions and states and applies a linear layer on top.\"\"\"\n",
        "\n",
        "    def __init__(self, state_embedder, instruction_embedder, embed_dim):\n",
        "        super().__init__(embed_dim)\n",
        "        self._state_embedder = state_embedder\n",
        "        self._instruction_embedder = instruction_embedder\n",
        "        if instruction_embedder is not None:\n",
        "            self._final_layer = nn.Linear(state_embedder.embed_dim + instruction_embedder.embed_dim, embed_dim)\n",
        "            assert self._state_embedder.embed_dim == embed_dim\n",
        "\n",
        "    def forward(self, states):\n",
        "        state_embeds = self._state_embedder([state.observation for state in states])\n",
        "        if self._instruction_embedder is not None:\n",
        "            instruction_embeds = self._instruction_embedder([torch.tensor(state.instructions) for state in states])\n",
        "            return self._final_layer(F.relu(torch.cat((state_embeds, instruction_embeds), -1)))\n",
        "        return state_embeds\n",
        "\n",
        "\n",
        "def init(module, weight_init, bias_init, gain=1):\n",
        "    weight_init(module.weight.data, gain=gain)\n",
        "    bias_init(module.bias.data)\n",
        "    return module\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "\n",
        "class MiniWorldEmbedder(Embedder):\n",
        "    \"\"\"Embeds 80x60 MiniWorld inputs.\n",
        "\n",
        "  Network taken from gym-miniworld/.\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, observation_space, embed_dim):\n",
        "        super().__init__(embed_dim)\n",
        "\n",
        "        # Architecture from gym-miniworld\n",
        "        # For 80x60 input\n",
        "        num_inputs = observation_space.shape[0]\n",
        "\n",
        "        self._network = nn.Sequential(\n",
        "            nn.Conv2d(num_inputs, 32, kernel_size=5, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=5, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            Flatten(),\n",
        "            nn.Linear(32 * 7 * 5, embed_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, obs):\n",
        "        # (batch_size, 80, 60, 3)\n",
        "        tensor = torch.stack(obs) / 255.0\n",
        "        return self._network(tensor)\n",
        "\n",
        "\n",
        "class SimpleGridStateEmbedder(Embedder):\n",
        "    \"\"\"Embedder for SimpleGridEnv states.\n",
        "\n",
        "  Concretely, embeds (x, y) separately with different embeddings for each cell.\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, observation_space, embed_dim):\n",
        "        \"\"\"Constructs for SimpleGridEnv.\n",
        "\n",
        "    Args:\n",
        "      observation_space (spaces.Box): limits for the observations to embed.\n",
        "    \"\"\"\n",
        "        super().__init__(embed_dim)\n",
        "\n",
        "        assert all(dim == 0 for dim in observation_space.low)\n",
        "        assert observation_space.dtype == np.int\n",
        "\n",
        "        hidden_size = 32\n",
        "        self._embedders = nn.ModuleList([nn.Embedding(dim, hidden_size) for dim in observation_space.high])\n",
        "        self._fc_layer = nn.Linear(hidden_size * len(observation_space.high), 256)\n",
        "        self._final_fc_layer = nn.Linear(256, embed_dim)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        tensor = torch.stack(obs)\n",
        "        embeds = []\n",
        "        for i in range(tensor.shape[1]):\n",
        "            embeds.append(self._embedders[i](tensor[:, i]))\n",
        "        return self._final_fc_layer(F.relu(self._fc_layer(torch.cat(embeds, -1))))\n",
        "\n",
        "\n",
        "class IDEmbedder(Embedder):\n",
        "    \"\"\"Embeds N-dim IDs by embedding each component and applying a linear\n",
        "  layer.\"\"\"\n",
        "\n",
        "    def __init__(self, observation_space, embed_dim):\n",
        "        \"\"\"Constructs for SimpleGridEnv.\n",
        "\n",
        "    Args:\n",
        "      observation_space (np.array): discrete max limits for each dimension of the\n",
        "        state (expects min is 0).\n",
        "    \"\"\"\n",
        "        super().__init__(embed_dim)\n",
        "\n",
        "        hidden_size = 32\n",
        "        self._embedders = nn.ModuleList([nn.Embedding(dim, hidden_size) for dim in observation_space])\n",
        "        self._fc_layer = nn.Linear(hidden_size * len(observation_space), embed_dim)\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config, observation_space):\n",
        "        return cls(observation_space, config.get(\"embed_dim\"))\n",
        "\n",
        "    def forward(self, obs):\n",
        "        tensor = obs\n",
        "        if len(tensor.shape) == 1:  # 1-d IDs\n",
        "            tensor = tensor.unsqueeze(-1)\n",
        "\n",
        "        embeds = []\n",
        "        for i in range(tensor.shape[1]):\n",
        "            embeds.append(self._embedders[i](tensor[:, i]))\n",
        "        return self._fc_layer(torch.cat(embeds, -1))\n",
        "\n",
        "\n",
        "class FixedVocabEmbedder(Embedder):\n",
        "    \"\"\"Wrapper around nn.Embedding obeying the Embedder interface.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        \"\"\"Constructs.\n",
        "\n",
        "    Args:\n",
        "      vocab_size (int): number of unique embeddings.\n",
        "      embed_dim (int): dimension of output embedding.\n",
        "    \"\"\"\n",
        "        super().__init__(embed_dim)\n",
        "\n",
        "        self._embedder = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(config.get(\"vocab_size\"), config.get(\"embed_dim\"))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Embeds inputs according to the underlying nn.Embedding.\n",
        "\n",
        "    Args:\n",
        "      inputs (list[int]): list of inputs of length batch.\n",
        "\n",
        "    Returns:\n",
        "      embedding (torch.Tensor): of shape (batch, embed_dim)\n",
        "    \"\"\"\n",
        "        tensor_inputs = torch.tensor(np.stack(inputs)).long()\n",
        "        return self._embedder(tensor_inputs)\n",
        "\n",
        "\n",
        "class LinearEmbedder(Embedder):\n",
        "    \"\"\"Wrapper around nn.Linear obeying the Embedder interface.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, embed_dim):\n",
        "        \"\"\"Wraps a nn.Linear(input_dim, embed_dim).\n",
        "\n",
        "    Args:\n",
        "      input_dim (int): dimension of inputs to embed.\n",
        "      embed_dim (int): dimension of output embedding.\n",
        "    \"\"\"\n",
        "        super().__init__(embed_dim)\n",
        "\n",
        "        self._embedder = nn.Linear(input_dim, embed_dim)\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(config.get(\"input_dim\"), config.get(\"embed_dim\"))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Embeds inputs according to the underlying nn.Linear.\n",
        "\n",
        "    Args:\n",
        "      inputs (list[np.array]): list of inputs of length batch.\n",
        "        Each input is an array of shape (input_dim).\n",
        "\n",
        "    Returns:\n",
        "      embedding (torch.Tensor): of shape (batch, embed_dim)\n",
        "    \"\"\"\n",
        "        inputs = np.stack(inputs)\n",
        "        if len(inputs.shape) == 1:\n",
        "            inputs = np.expand_dims(inputs, 1)\n",
        "        tensor_inputs = torch.tensor(inputs).float()\n",
        "        return self._embedder(tensor_inputs)\n",
        "\n",
        "\n",
        "class ExperienceEmbedder(Embedder):\n",
        "    \"\"\"Optionally embeds each of:\n",
        "\n",
        "    - state s\n",
        "    - instructions i\n",
        "    - actions a\n",
        "    - rewards r\n",
        "    - done d\n",
        "\n",
        "  Then passes a single linear layer over their concatenation.\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, state_embedder, instruction_embedder, action_embedder, reward_embedder, done_embedder, embed_dim\n",
        "    ):\n",
        "        \"\"\"Constructs.\n",
        "\n",
        "    Args:\n",
        "      state_embedder (Embedder | None)\n",
        "      instruction_embedder (Embedder | None)\n",
        "      action_embedder (Embedder | None)\n",
        "      reward_embedder (Embedder | None)\n",
        "      done_embedder (Embedder | None)\n",
        "      embed_dim (int): dimension of the output\n",
        "    \"\"\"\n",
        "        super().__init__(embed_dim)\n",
        "\n",
        "        self._embedders = collections.OrderedDict()\n",
        "        if state_embedder is not None:\n",
        "            self._embedders[\"state\"] = state_embedder\n",
        "        if instruction_embedder is not None:\n",
        "            self._embedders[\"instruction\"] = instruction_embedder\n",
        "        if action_embedder is not None:\n",
        "            self._embedders[\"action\"] = action_embedder\n",
        "        if reward_embedder is not None:\n",
        "            self._embedders[\"reward\"] = reward_embedder\n",
        "        if done_embedder is not None:\n",
        "            self._embedders[\"done\"] = done_embedder\n",
        "\n",
        "        # Register the embedders so they get gradients\n",
        "        self._register_embedders = nn.ModuleList(self._embedders.values())\n",
        "        self._final_layer = nn.Linear(sum(embedder.embed_dim for embedder in self._embedders.values()), embed_dim)\n",
        "\n",
        "    def forward(self, instruction_states):\n",
        "        \"\"\"Embeds the components for which this has embedders.\n",
        "\n",
        "    Args:\n",
        "      instruction_states (list[InstructionState]): batch of states.\n",
        "\n",
        "    Returns:\n",
        "      embedding (torch.Tensor): of shape (batch, embed_dim)\n",
        "    \"\"\"\n",
        "\n",
        "        def get_inputs(key, states):\n",
        "            if key == \"state\":\n",
        "                return [state.observation for state in states]\n",
        "            elif key == \"instruction\":\n",
        "                return [torch.tensor(state.instructions) for state in states]\n",
        "            elif key == \"action\":\n",
        "                actions = np.array([state.prev_action if state.prev_action is not None else -1 for state in states])\n",
        "                return actions + 1\n",
        "            elif key == \"reward\":\n",
        "                return [state.prev_reward for state in states]\n",
        "            elif key == \"done\":\n",
        "                return [state.done for state in states]\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported key: {}\".format(key))\n",
        "\n",
        "        embeddings = []\n",
        "        for key, embedder in self._embedders.items():\n",
        "            inputs = get_inputs(key, instruction_states)\n",
        "            embeddings.append(embedder(inputs))\n",
        "        return self._final_layer(F.relu(torch.cat(embeddings, -1)))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaYWscTWTnIf"
      },
      "source": [
        "# dqn.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0l94hP6Gi1m"
      },
      "source": [
        "# dqn.py\n",
        "import collections\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn import utils as torch_utils\n",
        "import schedule\n",
        "import replay\n",
        "import utils\n",
        "\n",
        "\n",
        "class DQNAgent(object):\n",
        "    @classmethod\n",
        "    def from_config(cls, config, env):\n",
        "        dqn = DQNPolicy.from_config(config.get(\"policy\"), env)\n",
        "        replay_buffer = replay.ReplayBuffer.from_config(config.get(\"buffer\"))\n",
        "        optimizer = optim.Adam(dqn.parameters(), lr=config.get(\"learning_rate\"))\n",
        "        return cls(\n",
        "            dqn,\n",
        "            replay_buffer,\n",
        "            optimizer,\n",
        "            config.get(\"sync_target_freq\"),\n",
        "            config.get(\"min_buffer_size\"),\n",
        "            config.get(\"batch_size\"),\n",
        "            config.get(\"update_freq\"),\n",
        "            config.get(\"max_grad_norm\"),\n",
        "        )\n",
        "\n",
        "    def __init__(\n",
        "        self, dqn, replay_buffer, optimizer, sync_freq, min_buffer_size, batch_size, update_freq, max_grad_norm\n",
        "    ):\n",
        "        \"\"\"\n",
        "    Args:\n",
        "      dqn (DQNPolicy)\n",
        "      replay_buffer (ReplayBuffer)\n",
        "      optimizer (torch.Optimizer)\n",
        "      sync_freq (int): number of updates between syncing the\n",
        "        DQN target Q network\n",
        "      min_buffer_size (int): replay buffer must be at least this large\n",
        "        before taking grad updates\n",
        "      batch_size (int): number of experience to sample per grad step\n",
        "      update_freq (int): number of update calls per parameter update.\n",
        "      max_grad_norm (float): gradient is clipped to this norm on each\n",
        "        update\n",
        "    \"\"\"\n",
        "        self._dqn = dqn\n",
        "        self._replay_buffer = replay_buffer\n",
        "        self._optimizer = optimizer\n",
        "        self._sync_freq = sync_freq\n",
        "        self._min_buffer_size = min_buffer_size\n",
        "        self._batch_size = batch_size\n",
        "        self._update_freq = update_freq\n",
        "        self._max_grad_norm = max_grad_norm\n",
        "        self._updates = 0\n",
        "\n",
        "        self._losses = collections.deque(maxlen=100)\n",
        "        self._grad_norms = collections.deque(maxlen=100)\n",
        "\n",
        "    def update(self, experience):\n",
        "        \"\"\"Updates agent on this experience.\n",
        "\n",
        "    Args:\n",
        "      experience (Experience): experience to update on.\n",
        "    \"\"\"\n",
        "        self._replay_buffer.add(experience)\n",
        "\n",
        "        if len(self._replay_buffer) >= self._min_buffer_size:\n",
        "            if self._updates % self._update_freq == 0:\n",
        "                experiences = self._replay_buffer.sample(self._batch_size)\n",
        "\n",
        "                self._optimizer.zero_grad()\n",
        "                loss = self._dqn.loss(experiences, np.ones(self._batch_size))\n",
        "                loss.backward()\n",
        "                self._losses.append(loss.item())\n",
        "\n",
        "                # clip according to the max allowed grad norm\n",
        "                grad_norm = torch_utils.clip_grad_norm_(self._dqn.parameters(), self._max_grad_norm, norm_type=2)\n",
        "                self._grad_norms.append(grad_norm)\n",
        "                self._optimizer.step()\n",
        "\n",
        "            if self._updates % self._sync_freq == 0:\n",
        "                self._dqn.sync_target()\n",
        "\n",
        "        self._updates += 1\n",
        "\n",
        "    def act(self, state, prev_hidden_state=None, test=False):\n",
        "        \"\"\"Given the current state, returns an action.\n",
        "\n",
        "    Args:\n",
        "      state (State)\n",
        "\n",
        "    Returns:\n",
        "      action (int)\n",
        "      hidden_state (object)\n",
        "    \"\"\"\n",
        "        return self._dqn.act(state, prev_hidden_state=prev_hidden_state, test=test)\n",
        "\n",
        "    @property\n",
        "    def stats(self):\n",
        "        def mean_with_default(l, default):\n",
        "            if len(l) == 0:\n",
        "                return default\n",
        "            return np.mean(l)\n",
        "\n",
        "        stats = self._dqn.stats\n",
        "        stats[\"loss\"] = mean_with_default(self._losses, None)\n",
        "        stats[\"grad_norm\"] = mean_with_default(self._grad_norms, None)\n",
        "        return {\"DQN/{}\".format(k): v for k, v in stats.items()}\n",
        "\n",
        "    def state_dict(self):\n",
        "        \"\"\"Returns a serializable dictionary containing all the relevant\n",
        "    details from the class.\n",
        "\n",
        "    Returns:\n",
        "      state_dict (dict)\n",
        "    \"\"\"\n",
        "        # Currently doesn't serialize replay buffer to save memory\n",
        "        return {\n",
        "            \"dqn\": self._dqn.state_dict(),\n",
        "            # \"replay_buffer\": self._replay_buffer,\n",
        "            \"optimizer\": self._optimizer.state_dict(),\n",
        "            \"sync_freq\": self._sync_freq,\n",
        "            \"min_buffer_size\": self._min_buffer_size,\n",
        "            \"batch_size\": self._batch_size,\n",
        "            \"update_freq\": self._update_freq,\n",
        "            \"max_grad_norm\": self._max_grad_norm,\n",
        "            \"updates\": self._updates,\n",
        "        }\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self._dqn.load_state_dict(state_dict[\"dqn\"])\n",
        "        # self._replay_buffer = state_dict[\"replay_buffer\"]\n",
        "        self._optimizer.load_state_dict(state_dict[\"optimizer\"])\n",
        "        self._sync_freq = state_dict[\"sync_freq\"]\n",
        "        self._min_buffer_size = state_dict[\"min_buffer_size\"]\n",
        "        self._batch_size = state_dict[\"batch_size\"]\n",
        "        self._update_freq = state_dict[\"update_freq\"]\n",
        "        self._max_grad_norm = state_dict[\"max_grad_norm\"]\n",
        "        self._updates = state_dict[\"updates\"]\n",
        "\n",
        "    def set_reward_relabeler(self, reward_relabeler):\n",
        "        \"\"\"See DQNPolicy.reward_relabeler.\"\"\"\n",
        "        self._dqn.set_reward_relabeler(reward_relabeler)\n",
        "\n",
        "\n",
        "class DQNPolicy(nn.Module):\n",
        "    @classmethod\n",
        "    def from_config(cls, config, env):\n",
        "        def embedder_factory():\n",
        "            embedder_config = config.get(\"embedder\")\n",
        "            embed_type = embedder_config.get(\"type\")\n",
        "            if embed_type == \"instruction\":\n",
        "                return InstructionPolicyEmbedder.from_config(embedder_config, env)\n",
        "            elif embed_type == \"recurrent\":\n",
        "                return RecurrentStateEmbedder.from_config(embedder_config, env)\n",
        "            elif embedder_config.get(\"type\") == \"varibad\":\n",
        "                return VariBADEmbedder.from_config(embedder_config, env)\n",
        "            elif embedder_config.get(\"type\") == \"import\":\n",
        "                return RecurrentAndTaskIDEmbedder.from_config(embedder_config, env)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported embedding type: {}\".format(embed_type))\n",
        "\n",
        "        policy_type = config.get(\"type\")\n",
        "        if policy_type == \"vanilla\":\n",
        "            pass\n",
        "        elif policy_type == \"recurrent\":\n",
        "            cls = RecurrentDQNPolicy\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported policy type: {}\".format(policy_type))\n",
        "\n",
        "        epsilon_schedule = schedule.LinearSchedule.from_config(config.get(\"epsilon_schedule\"))\n",
        "        return cls(\n",
        "            env.action_space.n,\n",
        "            epsilon_schedule,\n",
        "            config.get(\"test_epsilon\"),\n",
        "            embedder_factory,\n",
        "            config.get(\"discount\"),\n",
        "        )\n",
        "\n",
        "    # notice the above class method serves as input of the following\n",
        "\n",
        "    def __init__(self, num_actions, epsilon_schedule, test_epsilon, state_embedder_factory, gamma=0.99):\n",
        "        \"\"\"DQNPolicy should typically be constructed via from_config, and not\n",
        "    through the constructor.\n",
        "\n",
        "    Args:\n",
        "      num_actions (int): the number of possible actions to take at each\n",
        "        state\n",
        "      epsilon_schedule (Schedule): defines rate at which epsilon decays\n",
        "      test_epsilon (float): epsilon to use during test time (when test is\n",
        "        True in act)\n",
        "      state_embedder_factory (Callable --> StateEmbedder): type of state\n",
        "        embedder to use\n",
        "      gamma (float): discount factor\n",
        "    \"\"\"\n",
        "        super().__init__()\n",
        "        self._Q = DuelingNetwork(num_actions, state_embedder_factory())\n",
        "        self._target_Q = DuelingNetwork(num_actions, state_embedder_factory())\n",
        "        self._num_actions = num_actions\n",
        "        self._epsilon_schedule = epsilon_schedule\n",
        "        self._test_epsilon = test_epsilon\n",
        "        self._gamma = gamma\n",
        "        self._reward_relabeler = None\n",
        "\n",
        "        # Used for generating statistics about the policy\n",
        "        # Average of max Q values\n",
        "        self._max_q = collections.deque(maxlen=1000)\n",
        "        self._min_q = collections.deque(maxlen=1000)\n",
        "        self._losses = collections.defaultdict(lambda: collections.deque(maxlen=1000))\n",
        "\n",
        "    def act(self, state, prev_hidden_state=None, test=False):\n",
        "        \"\"\"\n",
        "    Args:\n",
        "      state (State)\n",
        "      test (bool): if True, takes on the test epsilon value\n",
        "      prev_hidden_state (object | None): unused agent state.\n",
        "      epsilon (float | None): if not None, overrides the epsilon greedy\n",
        "      schedule with this epsilon value. Mutually exclusive with test\n",
        "      flag\n",
        "\n",
        "    Returns:\n",
        "      int: action\n",
        "      hidden_state (None)\n",
        "    \"\"\"\n",
        "        del prev_hidden_state\n",
        "\n",
        "        q_values, hidden_state = self._Q([state], None)\n",
        "        if test:\n",
        "            epsilon = self._test_epsilon\n",
        "        else:\n",
        "            epsilon = self._epsilon_schedule.step()\n",
        "        self._max_q.append(torch.max(q_values).item())\n",
        "        self._min_q.append(torch.min(q_values).item())\n",
        "        return epsilon_greedy(q_values, epsilon)[0], None\n",
        "\n",
        "    def loss(self, experiences, weights):\n",
        "        \"\"\"Updates parameters from a batch of experiences\n",
        "\n",
        "    Minimizing the loss:\n",
        "\n",
        "      (target - Q(s, a))^2\n",
        "\n",
        "      target = r if done\n",
        "           r + \\gamma * max_a' Q(s', a')\n",
        "\n",
        "    Args:\n",
        "      experiences (list[Experience]): batch of experiences, state and\n",
        "        next_state may be LazyFrames or np.arrays\n",
        "      weights (list[float]): importance weights on each experience\n",
        "\n",
        "    Returns:\n",
        "      loss (torch.tensor): MSE loss on the experiences.\n",
        "    \"\"\"\n",
        "        batch_size = len(experiences)\n",
        "        states = [e.state for e in experiences]\n",
        "        actions = torch.tensor([e.action for e in experiences]).long()\n",
        "        next_states = [e.next_state for e in experiences]\n",
        "        rewards = torch.tensor([e.reward for e in experiences]).float()\n",
        "\n",
        "        # (batch_size,) 1 if was not done, otherwise 0\n",
        "        not_done_mask = torch.tensor([1 - e.done for e in experiences]).byte()\n",
        "        weights = torch.tensor(weights).float()\n",
        "\n",
        "        current_state_q_values, aux_losses = self._Q(states, None)\n",
        "        if isinstance(aux_losses, dict):\n",
        "            for name, loss in aux_losses.items():\n",
        "                self._losses[name].append(loss.detach().cpu().data.numpy())\n",
        "        current_state_q_values = current_state_q_values.gather(1, actions.unsqueeze(1))\n",
        "\n",
        "        # DDQN\n",
        "        best_actions = torch.max(self._Q(next_states, None)[0], 1)[1].unsqueeze(1)\n",
        "        next_state_q_values = self._target_Q(next_states, None)[0].gather(1, best_actions).squeeze(1)\n",
        "        targets = rewards + self._gamma * (next_state_q_values * not_done_mask.float())\n",
        "        targets.detach_()  # Don't backprop through targets\n",
        "\n",
        "        td_error = current_state_q_values.squeeze() - targets\n",
        "        loss = torch.mean((td_error ** 2) * weights)\n",
        "        self._losses[\"td_error\"].append(loss.detach().cpu().data.numpy())\n",
        "        aux_loss = 0\n",
        "        if isinstance(aux_losses, dict):\n",
        "            aux_loss = sum(aux_losses.values())\n",
        "        return loss + aux_loss\n",
        "\n",
        "    def sync_target(self):\n",
        "        \"\"\"Syncs the target Q values with the current Q values\"\"\"\n",
        "        self._target_Q.load_state_dict(self._Q.state_dict())\n",
        "\n",
        "    def set_reward_relabeler(self, reward_relabeler):\n",
        "        \"\"\"Sets the reward relabeler when computing the loss.\n",
        "\n",
        "    Args:\n",
        "      reward_relabeler (RewardLabeler)\n",
        "\n",
        "    Raises:\n",
        "      ValueError: when the reward relabeler has already been set.\n",
        "    \"\"\"\n",
        "        if self._reward_relabeler is not None:\n",
        "            raise ValueError(\"Reward relabeler already set.\")\n",
        "        self._reward_relabeler = reward_relabeler\n",
        "\n",
        "    @property\n",
        "    def stats(self):\n",
        "        \"\"\"See comments in constructor for more details about what these stats\n",
        "    are\"\"\"\n",
        "\n",
        "        def mean_with_default(l, default):\n",
        "            if len(l) == 0:\n",
        "                return default\n",
        "            return np.mean(l)\n",
        "\n",
        "        stats = {\n",
        "            \"epsilon\": self._epsilon_schedule.step(take_step=False),\n",
        "            \"Max Q\": mean_with_default(self._max_q, None),\n",
        "            \"Min Q\": mean_with_default(self._min_q, None),\n",
        "        }\n",
        "        for name, losses in self._losses.items():\n",
        "            stats[name] = np.mean(losses)\n",
        "        return stats\n",
        "\n",
        "\n",
        "class RecurrentDQNPolicy(DQNPolicy):\n",
        "    \"\"\"Implements a DQN policy that uses an RNN on the observations.\"\"\"\n",
        "\n",
        "    def loss(self, experiences, weights):\n",
        "        \"\"\"Updates recurrent parameters from a batch of sequential experiences\n",
        "\n",
        "    Minimizing the DQN loss:\n",
        "\n",
        "      (target - Q(s, a))^2\n",
        "\n",
        "      target = r if done\n",
        "           r + \\gamma * max_a' Q(s', a')\n",
        "\n",
        "    Args:\n",
        "      experiences (list[list[Experience]]): batch of sequences of experiences.\n",
        "      weights (list[float]): importance weights on each experience\n",
        "\n",
        "    Returns:\n",
        "      loss (torch.tensor): MSE loss on the experiences.\n",
        "    \"\"\"\n",
        "        unpadded_experiences = experiences\n",
        "        experiences, mask = utils.pad(experiences)\n",
        "        batch_size = len(experiences)\n",
        "        seq_len = len(experiences[0])\n",
        "\n",
        "        hidden_states = [seq[0].agent_state for seq in experiences]\n",
        "        # Include the next states in here to minimize calls to _Q\n",
        "        states = [[e.state for e in seq] + [seq[-1].next_state] for seq in experiences]\n",
        "        actions = torch.tensor([e.action for seq in experiences for e in seq]).long()\n",
        "        next_hidden_states = [seq[0].next_agent_state for seq in experiences]\n",
        "        next_states = [[e.next_state for e in seq] for seq in experiences]\n",
        "        rewards = torch.tensor([e.reward for seq in experiences for e in seq]).float()\n",
        "\n",
        "        # Relabel the rewards on the fly\n",
        "        if self._reward_relabeler is not None:\n",
        "            trajectories = [seq[0].trajectory for seq in experiences]\n",
        "            # (batch_size, max_seq_len)\n",
        "            indices = torch.tensor([[e.index for e in seq] for seq in experiences]).long()\n",
        "\n",
        "            # (batch_size * max_trajectory_len)\n",
        "            rewards = self._reward_relabeler.label_rewards(trajectories)[0].gather(-1, indices).reshape(-1)\n",
        "\n",
        "        # (batch_size,) 1 if was not done, otherwise 0\n",
        "        not_done_mask = ~(torch.tensor([e.done for seq in experiences for e in seq]).bool())\n",
        "        weights = torch.tensor(weights).float()\n",
        "\n",
        "        # (batch_size, seq_len + 1, actions)\n",
        "        q_values, _ = self._Q(states, hidden_states)\n",
        "        current_q_values = q_values[:, :-1, :]\n",
        "        current_q_values = current_q_values.reshape(batch_size * seq_len, -1)\n",
        "        # (batch_size * seq_len, 1)\n",
        "        current_state_q_values = current_q_values.gather(1, actions.unsqueeze(1))\n",
        "\n",
        "        aux_losses = {}\n",
        "        if hasattr(self._Q._state_embedder, \"aux_loss\"):\n",
        "            aux_losses = self._Q._state_embedder.aux_loss(unpadded_experiences)\n",
        "            if isinstance(aux_losses, dict):\n",
        "                for name, loss in aux_losses.items():\n",
        "                    self._losses[name].append(loss.detach().cpu().data.numpy())\n",
        "\n",
        "        # DDQN\n",
        "        next_q_values = q_values[:, 1:, :]\n",
        "        # (batch_size * seq_len, actions)\n",
        "        next_q_values = next_q_values.reshape(batch_size * seq_len, -1)\n",
        "        best_actions = torch.max(next_q_values, 1)[1].unsqueeze(1)\n",
        "        # Using the same hidden states for target\n",
        "        target_q_values, _ = self._target_Q(next_states, next_hidden_states)\n",
        "        target_q_values = target_q_values.reshape(batch_size * seq_len, -1)\n",
        "        next_state_q_values = target_q_values.gather(1, best_actions).squeeze(1)\n",
        "        targets = rewards + self._gamma * (next_state_q_values * not_done_mask.float())\n",
        "        targets.detach_()  # Don't backprop through targets\n",
        "\n",
        "        td_error = current_state_q_values.squeeze() - targets\n",
        "        weights = weights.unsqueeze(1) * mask.float()\n",
        "        loss = (td_error ** 2).reshape(batch_size, seq_len) * weights\n",
        "        loss = loss.sum() / mask.sum()  # masked mean\n",
        "        return loss + sum(aux_losses.values())\n",
        "\n",
        "    def act(self, state, prev_hidden_state=None, test=False):\n",
        "        \"\"\"\n",
        "    Args:\n",
        "      state (State)\n",
        "      test (bool): if True, takes on the test epsilon value\n",
        "      prev_hidden_state (object | None): unused agent state.\n",
        "      epsilon (float | None): if not None, overrides the epsilon greedy\n",
        "      schedule with this epsilon value. Mutually exclusive with test\n",
        "      flag\n",
        "\n",
        "    Returns:\n",
        "      int: action\n",
        "      hidden_state (None)\n",
        "    \"\"\"\n",
        "        q_values, hidden_state = self._Q([[state]], prev_hidden_state)\n",
        "        if test:\n",
        "            epsilon = self._test_epsilon\n",
        "        else:\n",
        "            epsilon = self._epsilon_schedule.step()\n",
        "        self._max_q.append(torch.max(q_values).item())\n",
        "        self._min_q.append(torch.min(q_values).item())\n",
        "        return epsilon_greedy(q_values, epsilon)[0], hidden_state\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    \"\"\"Implements the Q-function.\"\"\"\n",
        "\n",
        "    def __init__(self, num_actions, state_embedder):\n",
        "        \"\"\"\n",
        "    Args:\n",
        "      num_actions (int): the number of possible actions at each state\n",
        "      state_embedder (StateEmbedder): the state embedder to use\n",
        "    \"\"\"\n",
        "        super(DQN, self).__init__()\n",
        "        self._state_embedder = state_embedder\n",
        "        self._q_values = nn.Linear(self._state_embedder.embed_dim, num_actions)\n",
        "\n",
        "    def forward(self, states, hidden_states=None):\n",
        "        \"\"\"Returns Q-values for each of the states.\n",
        "\n",
        "    Args:\n",
        "      states (FloatTensor): shape (batch_size, 84, 84, 4)\n",
        "      hidden_states (object | None): hidden state returned by previous call to\n",
        "        forward. Must be called on constiguous states.\n",
        "\n",
        "    Returns:\n",
        "      FloatTensor: (batch_size, num_actions)\n",
        "      hidden_state (object)\n",
        "    \"\"\"\n",
        "        state_embed, hidden_state = self._state_embedder(states, hidden_states)\n",
        "        return self._q_values(state_embed), hidden_state\n",
        "\n",
        "\n",
        "class DuelingNetwork(DQN):\n",
        "    \"\"\"Implements the following Q-network:\n",
        "\n",
        "    Q(s, a) = V(s) + A(s, a) - avg_a' A(s, a')\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, num_actions, state_embedder):\n",
        "        super(DuelingNetwork, self).__init__(num_actions, state_embedder)\n",
        "        self._V = nn.Linear(self._state_embedder.embed_dim, 1)\n",
        "        self._A = nn.Linear(self._state_embedder.embed_dim, num_actions)\n",
        "\n",
        "    def forward(self, states, hidden_states=None):\n",
        "        state_embedding, hidden_state = self._state_embedder(states, hidden_states)\n",
        "        V = self._V(state_embedding)\n",
        "        advantage = self._A(state_embedding)\n",
        "        mean_advantage = torch.mean(advantage)\n",
        "        return V + advantage - mean_advantage, hidden_state\n",
        "\n",
        "\n",
        "def epsilon_greedy(q_values, epsilon):\n",
        "    \"\"\"Returns the index of the highest q value with prob 1 - epsilon,\n",
        "  otherwise uniformly at random with prob epsilon.\n",
        "\n",
        "  Args:\n",
        "    q_values (Variable[FloatTensor]): (batch_size, num_actions)\n",
        "    epsilon (float)\n",
        "\n",
        "  Returns:\n",
        "    list[int]: actions\n",
        "  \"\"\"\n",
        "    batch_size, num_actions = q_values.size()\n",
        "    _, max_indices = torch.max(q_values, 1)\n",
        "    max_indices = max_indices.cpu().data.numpy()\n",
        "    actions = []\n",
        "    for i in range(batch_size):\n",
        "        if np.random.random() > epsilon:\n",
        "            actions.append(max_indices[i])\n",
        "        else:\n",
        "            actions.append(np.random.randint(0, num_actions))\n",
        "    return actions"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Srk8W8KVH1kw"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import tqdm.notebook as tqdm\n",
        "\n",
        "import config as cfg\n",
        "import grid\n",
        "import city\n",
        "import policy\n",
        "import relabel\n",
        "import rl\n",
        "import utils\n",
        "\n",
        "\n",
        "def run_episode(env, policy, experience_observers=None, test=False):\n",
        "    \"\"\"Runs a single episode on the environment following the policy.\n",
        "\n",
        "  Args:\n",
        "    env (gym.Environment): environment to run on.\n",
        "    policy (Policy): policy to follow.\n",
        "    experience_observers (list[Callable] | None): each observer is called with\n",
        "      with each experience at each timestep.\n",
        "\n",
        "  Returns:\n",
        "    episode (list[Experience]): experiences from the episode.\n",
        "    renders (list[object | None]): renderings of the episode, only rendered if\n",
        "      test=True. Otherwise, returns list of Nones.\n",
        "  \"\"\"\n",
        "    # Optimization: rendering takes a lot of time.\n",
        "    def maybe_render(env, action, reward, timestep):\n",
        "        if test:\n",
        "            render = env.render()\n",
        "            render.write_text(\"Action: {}\".format(str(action)))\n",
        "            render.write_text(\"Reward: {}\".format(reward))\n",
        "            render.write_text(\"Timestep: {}\".format(timestep))\n",
        "            return render\n",
        "        return None\n",
        "\n",
        "    if experience_observers is None:\n",
        "        experience_observers = []\n",
        "\n",
        "    episode = []\n",
        "    state = env.reset()\n",
        "    timestep = 0\n",
        "    renders = [maybe_render(env, None, 0, timestep)]\n",
        "    hidden_state = None\n",
        "    while True:\n",
        "        action, next_hidden_state = policy.act(state, hidden_state, test=test)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        timestep += 1\n",
        "        renders.append(maybe_render(env, grid.Action(action), reward, timestep))\n",
        "\n",
        "        # hidden states are embeddings of trajectory till now?\n",
        "        experience = rl.Experience(state, action, reward, next_state, done, info, hidden_state, next_hidden_state)\n",
        "        episode.append(experience)\n",
        "        for observer in experience_observers:\n",
        "            observer(experience)\n",
        "\n",
        "        state = next_state\n",
        "        hidden_state = next_hidden_state\n",
        "        if done:\n",
        "            return episode, renders\n",
        "\n",
        "\n",
        "def get_env_class(environment_type):\n",
        "    \"\"\"Returns the environment class specified by the type.\n",
        "\n",
        "  Args:\n",
        "    environment_type (str): a valid environment type.\n",
        "\n",
        "  Returns:\n",
        "    environment_class (type): type specified.\n",
        "  \"\"\"\n",
        "    if environment_type == \"vanilla\":\n",
        "        return city.CityGridEnv\n",
        "    elif environment_type == \"distraction\":\n",
        "        return city.DistractionGridEnv\n",
        "    elif environment_type == \"map\":\n",
        "        return city.MapGridEnv\n",
        "    elif environment_type == \"cooking\":\n",
        "        return cooking.CookingGridEnv\n",
        "    elif environment_type == \"miniworld_sign\":\n",
        "        # Dependencies on OpenGL, so only load if absolutely necessary\n",
        "        from envs.miniworld import sign\n",
        "\n",
        "        return sign.MiniWorldSign\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported environment type: {}\".format(environment_type))\n",
        "\n",
        "\n",
        "def get_instruction_agent(instruction_config, instruction_env):\n",
        "    if instruction_config.get(\"type\") == \"learned\":\n",
        "        return DQNAgent.from_config(instruction_config, instruction_env)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid instruction agent: {}\".format(instruction_config.get(\"type\")))\n",
        "\n",
        "\n",
        "def get_exploration_agent(exploration_config, exploration_env):\n",
        "    if exploration_config.get(\"type\") == \"learned\":\n",
        "        return DQNAgent.from_config(exploration_config, exploration_env)\n",
        "    elif exploration_config.get(\"type\") == \"random\":\n",
        "        return policy.RandomPolicy(exploration_env.action_space)\n",
        "    elif exploration_config.get(\"type\") == \"none\":\n",
        "        return policy.ConstantActionPolicy(grid.Action.end_episode)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid exploration agent: {}\".format(exploration_config.get(\"type\")))\n",
        "\n",
        "\n",
        "def log_episode(exploration_episode, exploration_rewards, distances, path):\n",
        "    with open(path, \"w+\") as f:\n",
        "        f.write(\"Env ID: {}\\n\".format(exploration_episode[0].state.env_id))\n",
        "        for t, (exp, exploration_reward, distance) in enumerate(\n",
        "            zip(exploration_episode, exploration_rewards, distances)\n",
        "        ):\n",
        "            f.write(\"=\" * 80 + \"\\n\")\n",
        "            f.write(\"Timestep: {}\\n\".format(t))\n",
        "            f.write(\"State: {}\\n\".format(exp.state.observation))\n",
        "            f.write(\"Action: {}\\n\".format(grid.Action(exp.action).name))\n",
        "            f.write(\"Reward: {}\\n\".format(exploration_reward))\n",
        "            f.write(\"Distance: {}\\n\".format(distance))\n",
        "            f.write(\"Next state: {}\\n\".format(exp.next_state.observation))\n",
        "            f.write(\"=\" * 80 + \"\\n\")\n",
        "            f.write(\"\\n\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn-SzZ-ZTp4l"
      },
      "source": [
        "# configs.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjwdtyT5GdQS"
      },
      "source": [
        "# configs.py\n",
        "import json\n",
        "\n",
        "\n",
        "_dream_config_tree = {\n",
        "  \"environment\": \"vanilla\",\n",
        "  \"instruction_agent\": {\n",
        "    \"type\": \"learned\",\n",
        "    \"policy\": {\n",
        "      \"type\": \"vanilla\",\n",
        "      \"epsilon_schedule\": {\n",
        "        \"begin\": 1.0,\n",
        "        \"end\": 0.01,\n",
        "        \"total_steps\": 250000\n",
        "      },\n",
        "      \"embedder\": {\n",
        "        \"type\": \"instruction\",\n",
        "        \"obs_embedder\": {\n",
        "          \"embed_dim\": 64\n",
        "        },\n",
        "        \"instruction_embedder\": {\n",
        "          \"embed_dim\": 64\n",
        "        },\n",
        "        \"transition_embedder\": {\n",
        "          \"state_embed_dim\": 64,\n",
        "          \"action_embed_dim\": 32,\n",
        "          \"embed_dim\": 64\n",
        "        },\n",
        "        \"trajectory_embedder\": {\n",
        "          \"type\": \"ours\",\n",
        "          \"penalty\": 0.1\n",
        "        },\n",
        "        \"attention_query_dim\": 64,\n",
        "        \"embed_dim\": 64\n",
        "      },\n",
        "      \"test_epsilon\": 0,\n",
        "      \"discount\": 0.99\n",
        "    },\n",
        "    \"buffer\": {\n",
        "      \"type\": \"vanilla\",\n",
        "      \"max_buffer_size\": 100000\n",
        "    },\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"sync_target_freq\": 5000,\n",
        "    \"min_buffer_size\": 500,\n",
        "    \"batch_size\": 32,\n",
        "    \"update_freq\": 4,\n",
        "    \"max_grad_norm\": 10\n",
        "  },\n",
        "  \"exploration_agent\": {\n",
        "    \"type\": \"learned\",\n",
        "    # \"type\": \"random\",\n",
        "    \"policy\": {\n",
        "      \"type\": \"recurrent\",\n",
        "      \"epsilon_schedule\": {\n",
        "        \"begin\": 1.0,\n",
        "        \"end\": 0.01,\n",
        "        \"total_steps\": 250000\n",
        "      },\n",
        "      \"embedder\": {\n",
        "        \"type\": \"recurrent\",\n",
        "        \"experience_embedder\": {\n",
        "          \"state_embed_dim\": 64,\n",
        "          \"action_embed_dim\": 16,\n",
        "          \"embed_dim\": 64\n",
        "        },\n",
        "        \"embed_dim\": 64\n",
        "      },\n",
        "      \"test_epsilon\": 0,\n",
        "      \"discount\": 0.99\n",
        "    },\n",
        "    \"buffer\": {\n",
        "      \"type\": \"sequential\",\n",
        "      \"max_buffer_size\": 16000,\n",
        "      \"sequence_length\": 50\n",
        "    },\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"sync_target_freq\": 5000,\n",
        "    \"min_buffer_size\": 8000,\n",
        "    \"batch_size\": 32,\n",
        "    \"update_freq\": 4,\n",
        "    \"max_grad_norm\": 10\n",
        "  }\n",
        "}\n",
        "\n",
        "with open(\"./dream.json\", \"w\") as f:\n",
        "  json.dump(_dream_config_tree, f, indent=4, sort_keys=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uuA6SuHUCNx"
      },
      "source": [
        "# dream.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UZzQJXIHMNc"
      },
      "source": [
        "%tensorboard --logdir ./DREAM/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8z_y3fFP_m_"
      },
      "source": [
        "!zip -r /content/dream.zip /content/DREAM\n",
        "from google.colab import files\n",
        "files.download(\"/content/dream.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hehNZfIVHMK_",
        "outputId": "8e0ab743-8169-42d5-815f-25216b5eed7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "848f8b0ce97146ad94f4b8fe03dee7f0",
            "5890cb4bab06467b80e5628e39f2d744",
            "513c8b20756540969553c0c1cc3e8d90",
            "a4a9c64f8cff49bdba157efebc97bc47",
            "edfedf22bb3549899eaedfb5f134bf34",
            "6547f26062d44527a6b75651867136ff",
            "38078b2e31b64ed39b9accf4cd2f4300",
            "d1dba36b42bf40b292910e8d67899c0f",
            "27416dfc210e4fea871c003576846669",
            "483c06ac41b64f2f99dd1cf6885fc973",
            "051aa1a30d69434e81851bffb8698ce6"
          ]
        }
      },
      "source": [
        "# dream.py\n",
        "import argparse\n",
        "import collections\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import tqdm.notebook as tqdm\n",
        "\n",
        "import config as cfg\n",
        "import grid\n",
        "import policy\n",
        "import relabel\n",
        "import rl\n",
        "import utils\n",
        "\n",
        "MAX_EPISODES = 19999\n",
        "\n",
        "\n",
        "def main():\n",
        "    arg_parser = argparse.ArgumentParser()\n",
        "    arg_parser.add_argument(\"-c\", \"--configs\", action=\"append\", default=[\"./dream.json\"])\n",
        "    arg_parser.add_argument(\n",
        "        \"-b\", \"--config_bindings\", action=\"append\", default=[], help=\"bindings to overwrite in the configs.\"\n",
        "    )\n",
        "    arg_parser.add_argument(\"-x\", \"--base_dir\", default=\"experiments\", help=\"directory to log experiments\")\n",
        "    arg_parser.add_argument(\n",
        "        \"-p\", \"--checkpoint\", default=None, help=\"path to checkpoint directory to load from or None\"\n",
        "    )\n",
        "    arg_parser.add_argument(\n",
        "        \"-f\",\n",
        "        \"--force_overwrite\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Overwrites experiment under this experiment name, if it exists.\",\n",
        "    )\n",
        "    arg_parser.add_argument(\"-s\", \"--seed\", default=0, help=\"random seed to use.\", type=int)\n",
        "    arg_parser.add_argument(\"exp_name\", help=\"name of the experiment to run\")\n",
        "    args = arg_parser.parse_args()\n",
        "    config = cfg.Config.from_files_and_bindings(args.configs, args.config_bindings)\n",
        "\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "\n",
        "    ######################### Change the logdir here\n",
        "    exp_agent = config.get(\"exploration_agent\").get(\"type\")\n",
        "    exp_dir = f\"./DREAM/{exp_agent}\"\n",
        "    #########################\n",
        "\n",
        "    if os.path.exists(exp_dir) and not args.force_overwrite:\n",
        "        raise ValueError(\"Experiment already exists at: {}\".format(exp_dir))\n",
        "    shutil.rmtree(exp_dir, ignore_errors=True)  # remove directory if exists\n",
        "    os.makedirs(exp_dir)\n",
        "\n",
        "    with open(os.path.join(exp_dir, \"config.json\"), \"w+\") as f:\n",
        "        config.to_file(f)\n",
        "    print(config)\n",
        "\n",
        "    env_class = get_env_class(config.get(\"environment\"))\n",
        "\n",
        "    # Use GPU if possible\n",
        "    device = torch.device(\"cpu\")\n",
        "    if torch.cuda.is_available():\n",
        "        torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "        device = torch.device(\"cuda:0\")\n",
        "\n",
        "    print(\"Device: {}\".format(device))\n",
        "    tb_writer = utils.EpisodeAndStepWriter(os.path.join(exp_dir, \"tensorboard\"))\n",
        "\n",
        "    text_dir = os.path.join(exp_dir, \"text\")\n",
        "    os.makedirs(text_dir)\n",
        "\n",
        "    checkpoint_dir = os.path.join(exp_dir, \"checkpoints\")\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "    create_env = env_class.create_env\n",
        "    exploration_env = create_env(0)\n",
        "    instruction_env = env_class.instruction_wrapper()(exploration_env, [])\n",
        "    instruction_config = config.get(\"instruction_agent\")\n",
        "    instruction_agent = get_instruction_agent(instruction_config, instruction_env)\n",
        "\n",
        "    exploration_config = config.get(\"exploration_agent\")\n",
        "    exploration_agent = get_exploration_agent(exploration_config, exploration_env)\n",
        "\n",
        "    # Should probably expose this more gracefully\n",
        "    trajectory_embedder = instruction_agent._dqn._Q._state_embedder._trajectory_embedder\n",
        "    exploration_agent.set_reward_relabeler(trajectory_embedder)\n",
        "\n",
        "    # Due to the above hack, the trajectory embedder is being loaded twice.\n",
        "    if args.checkpoint is not None:\n",
        "        print(\"Loading checkpoint: {}\".format(args.checkpoint))\n",
        "        instruction_agent.load_state_dict(torch.load(os.path.join(args.checkpoint, \"instruction.pt\")))\n",
        "        exploration_agent.load_state_dict(torch.load(os.path.join(args.checkpoint, \"exploration.pt\")))\n",
        "\n",
        "    batch_size = 32\n",
        "    rewards = collections.deque(maxlen=200)\n",
        "    relabel_rewards = collections.deque(maxlen=200)\n",
        "    exploration_lengths = collections.deque(maxlen=200)\n",
        "    exploration_steps = 0\n",
        "    instruction_steps = 0\n",
        "\n",
        "\n",
        "    ## define the number of max episodes allowed\n",
        "    \n",
        "\n",
        "    # run_episode: \"\"\"Runs a single episode on the environment following the policy.\n",
        "    # Args:\n",
        "    #     env (gym.Environment): environment to run on.\n",
        "    #     policy (Policy): policy to follow.\n",
        "    #     experience_observers (list[Callable] | None): each observer is called with\n",
        "    #     with each experience at each timestep.\n",
        "\n",
        "    # Returns:\n",
        "    #     episode (list[Experience]): experiences from the episode.\n",
        "    #     renders (list[object | None]): renderings of the episode, only rendered if\n",
        "    #     test=True. Otherwise, returns list of Nones.\n",
        "    # \"\"\"\n",
        "\n",
        "\n",
        "    for step in tqdm.tqdm(range(MAX_EPISODES)):\n",
        "        exploration_env = create_env(step)\n",
        "        # returned episode: see line 53 of run_episode and rl.Experience in rl.py\n",
        "        exploration_episode, _ = run_episode(\n",
        "            # Exploration episode gets ignored\n",
        "            env_class.instruction_wrapper()(exploration_env, [], seed=max(0, step - 1)),\n",
        "            exploration_agent,\n",
        "        )\n",
        "\n",
        "        # update after finishing the episode\n",
        "        # Needed to keep references to the trajectory and index for reward labeling\n",
        "        for index, exp in enumerate(exploration_episode):\n",
        "            exploration_agent.update(relabel.TrajectoryExperience(exp, exploration_episode, index))\n",
        "\n",
        "        exploration_steps += len(exploration_episode)\n",
        "        exploration_lengths.append(len(exploration_episode))\n",
        "\n",
        "        # Don't share same random seed between exploration env and instructions (what does the seed control?)\n",
        "        # Same env (dynamics) as exploration env? \n",
        "        instruction_env = env_class.instruction_wrapper()(exploration_env, exploration_episode, seed=step + 1)\n",
        "        \n",
        "        ####### updatig the exec\n",
        "        ####### alternates between using ID and the trajectory embedder output\n",
        "\n",
        "        if step % 2 == 0:\n",
        "            trajectory_embedder.use_ids(False)\n",
        "        # call experience_observers: update on every\n",
        "        # for observer in experience_observers:\n",
        "        #     observer(experience)\n",
        "        episode, _ = run_episode(instruction_env, instruction_agent, experience_observers=[instruction_agent.update])\n",
        "        instruction_steps += len(episode)\n",
        "        trajectory_embedder.use_ids(True)\n",
        "\n",
        "        rewards.append(sum(exp.reward for exp in episode))\n",
        "\n",
        "        # Log reward for exploration agent\n",
        "        exploration_rewards, distances = trajectory_embedder.label_rewards([exploration_episode])\n",
        "        exploration_rewards = exploration_rewards[0]\n",
        "        distances = distances[0]\n",
        "        relabel_rewards.append(exploration_rewards.sum().item())\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            path = os.path.join(text_dir, \"{}.txt\".format(step))\n",
        "            log_episode(exploration_episode, exploration_rewards, distances, path)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            for k, v in instruction_agent.stats.items():\n",
        "                if v is not None:\n",
        "                    tb_writer.add_scalar(\n",
        "                        \"{}_{}\".format(\"instruction\", k), v, step, exploration_steps + instruction_steps\n",
        "                    )\n",
        "\n",
        "            for k, v in exploration_agent.stats.items():\n",
        "                if v is not None:\n",
        "                    tb_writer.add_scalar(\n",
        "                        \"{}_{}\".format(\"exploration\", k), v, step, exploration_steps + instruction_steps\n",
        "                    )\n",
        "\n",
        "            tb_writer.add_scalar(\"steps/exploration\", exploration_steps, step, exploration_steps + instruction_steps)\n",
        "            tb_writer.add_scalar(\"steps/instruction\", instruction_steps, step, exploration_steps + instruction_steps)\n",
        "            tb_writer.add_scalar(\"reward/train\", np.mean(rewards), step, exploration_steps + instruction_steps)\n",
        "            tb_writer.add_scalar(\n",
        "                \"reward/exploration\", np.mean(relabel_rewards), step, exploration_steps + instruction_steps\n",
        "            )\n",
        "            tb_writer.add_scalar(\n",
        "                \"steps/exploration_per_episode\",\n",
        "                np.mean(exploration_lengths),\n",
        "                step,\n",
        "                exploration_steps + instruction_steps,\n",
        "            )\n",
        "        \n",
        "        ########\n",
        "        ########\n",
        "                    # Test Time!! #\n",
        "        ########\n",
        "        ########\n",
        "        if step % 2000 == 0:\n",
        "            visualize_dir = os.path.join(exp_dir, \"visualize\", str(step))\n",
        "            os.makedirs(visualize_dir, exist_ok=True)\n",
        "\n",
        "            test_rewards = []\n",
        "            test_exploration_lengths = []\n",
        "            trajectory_embedder.use_ids(False)\n",
        "            for test_index in tqdm.tqdm(range(100)):\n",
        "                exploration_env = create_env(test_index, test=True)\n",
        "                exploration_episode, exploration_render = run_episode(\n",
        "                    env_class.instruction_wrapper()(exploration_env, [], seed=max(0, test_index - 1), test=True),\n",
        "                    exploration_agent,\n",
        "                    test=True,\n",
        "                )\n",
        "                test_exploration_lengths.append(len(exploration_episode))\n",
        "\n",
        "                instruction_env = env_class.instruction_wrapper()(\n",
        "                    exploration_env, exploration_episode, seed=test_index + 1, test=True\n",
        "                )\n",
        "                episode, render = run_episode(instruction_env, instruction_agent, test=True)\n",
        "                test_rewards.append(sum(exp.reward for exp in episode))\n",
        "\n",
        "                if test_index < 10:\n",
        "                    frames = [frame.image() for frame in render]\n",
        "                    save_path = os.path.join(visualize_dir, \"{}-instruction.gif\".format(test_index))\n",
        "                    frames[0].save(\n",
        "                        save_path,\n",
        "                        save_all=True,\n",
        "                        append_images=frames[1:],\n",
        "                        duration=750,\n",
        "                        loop=0,\n",
        "                        optimize=True,\n",
        "                        quality=20,\n",
        "                    )\n",
        "\n",
        "                    frames = [frame.image() for frame in exploration_render]\n",
        "                    save_path = os.path.join(visualize_dir, \"{}-exploration.gif\".format(test_index))\n",
        "                    frames[0].save(\n",
        "                        save_path,\n",
        "                        save_all=True,\n",
        "                        append_images=frames[1:],\n",
        "                        duration=750,\n",
        "                        loop=0,\n",
        "                        optimize=True,\n",
        "                        quality=20,\n",
        "                    )\n",
        "\n",
        "            tb_writer.add_scalar(\"reward/test\", np.mean(test_rewards), step, exploration_steps + instruction_steps)\n",
        "            tb_writer.add_scalar(\n",
        "                \"steps/test_exploration_per_episode\",\n",
        "                np.mean(test_exploration_lengths),\n",
        "                step,\n",
        "                exploration_steps + instruction_steps,\n",
        "            )\n",
        "\n",
        "            # Visualize training split\n",
        "            visualize_dir = os.path.join(exp_dir, \"visualize\", str(step), \"train\")\n",
        "            os.makedirs(visualize_dir, exist_ok=True)\n",
        "            for train_index in tqdm.tqdm(range(10)):\n",
        "                exploration_env = create_env(train_index)\n",
        "                # Test flags here only refer to making agent act with test flag and\n",
        "                # not test split environments\n",
        "                exploration_episode, exploration_render = run_episode(\n",
        "                    env_class.instruction_wrapper()(exploration_env, [], seed=max(0, train_index - 1)),\n",
        "                    exploration_agent,\n",
        "                    test=True,\n",
        "                )\n",
        "\n",
        "                instruction_env = env_class.instruction_wrapper()(\n",
        "                    exploration_env, exploration_episode, seed=train_index + 1\n",
        "                )\n",
        "                episode, render = run_episode(instruction_env, instruction_agent, test=True)\n",
        "\n",
        "                frames = [frame.image() for frame in render]\n",
        "                save_path = os.path.join(visualize_dir, \"{}-instruction.gif\".format(train_index))\n",
        "                frames[0].save(save_path, save_all=True, append_images=frames[1:], duration=750, loop=0)\n",
        "\n",
        "                frames = [frame.image() for frame in exploration_render]\n",
        "                save_path = os.path.join(visualize_dir, \"{}-exploration.gif\".format(train_index))\n",
        "                frames[0].save(save_path, save_all=True, append_images=frames[1:], duration=750, loop=0)\n",
        "            trajectory_embedder.use_ids(True)\n",
        "\n",
        "            if exploration_steps + instruction_steps > int(5e6):\n",
        "                return\n",
        "\n",
        "        ########\n",
        "        ########\n",
        "                    # End of Test Time!! #\n",
        "        ########\n",
        "        ########\n",
        "\n",
        "        if step != 0 and step % 20000 == 0:\n",
        "            print(\"Saving checkpoint\")\n",
        "            save_dir = os.path.join(checkpoint_dir, str(step))\n",
        "            os.makedirs(save_dir)\n",
        "\n",
        "            torch.save(instruction_agent.state_dict(), os.path.join(save_dir, \"instruction.pt\"))\n",
        "            torch.save(exploration_agent.state_dict(), os.path.join(save_dir, \"exploration.pt\"))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"environment\": \"vanilla\",\n",
            "    \"exploration_agent\": {\n",
            "        \"batch_size\": 32,\n",
            "        \"buffer\": {\n",
            "            \"max_buffer_size\": 16000,\n",
            "            \"sequence_length\": 50,\n",
            "            \"type\": \"sequential\"\n",
            "        },\n",
            "        \"learning_rate\": 0.0001,\n",
            "        \"max_grad_norm\": 10,\n",
            "        \"min_buffer_size\": 8000,\n",
            "        \"policy\": {\n",
            "            \"discount\": 0.99,\n",
            "            \"embedder\": {\n",
            "                \"embed_dim\": 64,\n",
            "                \"experience_embedder\": {\n",
            "                    \"action_embed_dim\": 16,\n",
            "                    \"embed_dim\": 64,\n",
            "                    \"state_embed_dim\": 64\n",
            "                },\n",
            "                \"type\": \"recurrent\"\n",
            "            },\n",
            "            \"epsilon_schedule\": {\n",
            "                \"begin\": 1.0,\n",
            "                \"end\": 0.01,\n",
            "                \"total_steps\": 250000\n",
            "            },\n",
            "            \"test_epsilon\": 0,\n",
            "            \"type\": \"recurrent\"\n",
            "        },\n",
            "        \"sync_target_freq\": 5000,\n",
            "        \"type\": \"learned\",\n",
            "        \"update_freq\": 4\n",
            "    },\n",
            "    \"instruction_agent\": {\n",
            "        \"batch_size\": 32,\n",
            "        \"buffer\": {\n",
            "            \"max_buffer_size\": 100000,\n",
            "            \"type\": \"vanilla\"\n",
            "        },\n",
            "        \"learning_rate\": 0.0001,\n",
            "        \"max_grad_norm\": 10,\n",
            "        \"min_buffer_size\": 500,\n",
            "        \"policy\": {\n",
            "            \"discount\": 0.99,\n",
            "            \"embedder\": {\n",
            "                \"attention_query_dim\": 64,\n",
            "                \"embed_dim\": 64,\n",
            "                \"instruction_embedder\": {\n",
            "                    \"embed_dim\": 64\n",
            "                },\n",
            "                \"obs_embedder\": {\n",
            "                    \"embed_dim\": 64\n",
            "                },\n",
            "                \"trajectory_embedder\": {\n",
            "                    \"penalty\": 0.1,\n",
            "                    \"type\": \"ours\"\n",
            "                },\n",
            "                \"transition_embedder\": {\n",
            "                    \"action_embed_dim\": 32,\n",
            "                    \"embed_dim\": 64,\n",
            "                    \"state_embed_dim\": 64\n",
            "                },\n",
            "                \"type\": \"instruction\"\n",
            "            },\n",
            "            \"epsilon_schedule\": {\n",
            "                \"begin\": 1.0,\n",
            "                \"end\": 0.01,\n",
            "                \"total_steps\": 250000\n",
            "            },\n",
            "            \"test_epsilon\": 0,\n",
            "            \"type\": \"vanilla\"\n",
            "        },\n",
            "        \"sync_target_freq\": 5000,\n",
            "        \"type\": \"learned\",\n",
            "        \"update_freq\": 4\n",
            "    }\n",
            "}\n",
            "Device: cuda:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=19999.0), HTML(value='')))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "848f8b0ce97146ad94f4b8fe03dee7f0"
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-8-0a98c4c292bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-8-0a98c4c292bf>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mexploration_env\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# returned episode: see line 53 of run_episode and rl.Experience in rl.py\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         exploration_episode, _ = run_episode(\n\u001b[0m\u001b[0;32m    125\u001b[0m             \u001b[1;31m# Exploration episode gets ignored\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0menv_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstruction_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexploration_env\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-6-1595eb577f51>\u001b[0m in \u001b[0;36mrun_episode\u001b[1;34m(env, policy, experience_observers, test)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mhidden_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_hidden_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mtimestep\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-5-f88c4262d0f4>\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, state, prev_hidden_state, test)\u001b[0m\n\u001b[0;32m     95\u001b[0m       \u001b[0mhidden_state\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \"\"\"\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev_hidden_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprev_hidden_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-5-f88c4262d0f4>\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, state, prev_hidden_state, test)\u001b[0m\n\u001b[0;32m    412\u001b[0m       \u001b[0mhidden_state\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m     \"\"\"\n\u001b[1;32m--> 414\u001b[1;33m         \u001b[0mq_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Q\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev_hidden_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    415\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m             \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_test_epsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-5-f88c4262d0f4>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, states, hidden_states)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m         \u001b[0mstate_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state_embedder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m         \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_V\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[0madvantage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-4-d41afbe43d35>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, states, hidden_state)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[1;31m# (batch_size * sequence_len, embed_dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 666\u001b[1;33m         \u001b[0mstate_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state_embedder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflattened\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    667\u001b[0m         \u001b[0mstate_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_embeds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-4-d41afbe43d35>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, instruction_states)\u001b[0m\n\u001b[0;32m    974\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedder\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_embedders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstruction_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 976\u001b[1;33m             \u001b[0membeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    977\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-4-d41afbe43d35>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    801\u001b[0m         \u001b[0membeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 803\u001b[1;33m             \u001b[0membeds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_embedders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    804\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_fc_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fc_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         return F.embedding(\n\u001b[0m\u001b[0;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1850\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1852\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.cuda.IntTensor instead (while checking arguments for embedding)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-U6vzCpPxCD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}